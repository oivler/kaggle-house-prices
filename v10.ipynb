{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - Advanced Regression Techniques\n",
    "## SCORE: .12150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 CPU cores (leaving 1 free)\n",
      "Train: (1460, 81), Test: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_jobs = max(1, os.cpu_count() - 1)\n",
    "print(f\"Using {n_jobs} CPU cores (leaving 1 free)\")\n",
    "\n",
    "data_dir = 'house-prices-advanced-regression-techniques'\n",
    "train = pd.read_csv(f'{data_dir}/train.csv')\n",
    "test = pd.read_csv(f'{data_dir}/test.csv')\n",
    "\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train['SalePrice'].copy()\n",
    "test_ids = test['Id'].copy()\n",
    "\n",
    "train_idx = len(train)\n",
    "all_data = pd.concat([train.drop('SalePrice', axis=1), test], ignore_index=True)\n",
    "\n",
    "y_train_log = np.log1p(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "\n",
    "none_cols = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "             'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', \n",
    "             'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "\n",
    "for col in none_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col].fillna('None', inplace=True)\n",
    "\n",
    "if 'LotFrontage' in all_data.columns:\n",
    "    all_data['LotFrontage'].fillna(all_data['LotFrontage'].median(), inplace=True)\n",
    "if 'MasVnrType' in all_data.columns:\n",
    "    all_data['MasVnrType'].fillna('None', inplace=True)\n",
    "if 'MasVnrArea' in all_data.columns:\n",
    "    all_data['MasVnrArea'].fillna(0, inplace=True)\n",
    "if 'Electrical' in all_data.columns:\n",
    "    all_data['Electrical'].fillna(all_data['Electrical'].mode()[0], inplace=True)\n",
    "if 'GarageYrBlt' in all_data.columns:\n",
    "    all_data['GarageYrBlt'].fillna(all_data['YearBuilt'], inplace=True)\n",
    "\n",
    "numerical_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if all_data[col].isnull().sum() > 0:\n",
    "        all_data[col].fillna(0, inplace=True)\n",
    "\n",
    "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if all_data[col].isnull().sum() > 0:\n",
    "        all_data[col].fillna(all_data[col].mode()[0], inplace=True)\n",
    "\n",
    "skewed_features = ['MiscVal', 'PoolArea', 'LotArea', '3SsnPorch', 'LowQualFinSF', \n",
    "                   'BsmtFinSF2', 'ScreenPorch', 'EnclosedPorch', 'MasVnrArea', \n",
    "                   'OpenPorchSF', 'LotFrontage', 'BsmtFinSF1', 'WoodDeckSF']\n",
    "for col in skewed_features:\n",
    "    if col in all_data.columns:\n",
    "        all_data[f'{col}_log'] = np.log1p(all_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(col in all_data.columns for col in ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']):\n",
    "    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "    all_data['TotalSF_log'] = np.log1p(all_data['TotalSF'])\n",
    "\n",
    "if all(col in all_data.columns for col in ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']):\n",
    "    all_data['TotalBathrooms'] = (all_data['FullBath'] + \n",
    "                                  all_data['HalfBath'] * 0.5 + \n",
    "                                  all_data['BsmtFullBath'] + \n",
    "                                  all_data['BsmtHalfBath'] * 0.5)\n",
    "\n",
    "if 'YrSold' in all_data.columns and 'YearBuilt' in all_data.columns:\n",
    "    all_data['HouseAge'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "    all_data['YearsSinceRemodel'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "    all_data['Remodeled'] = (all_data['YearBuilt'] != all_data['YearRemodAdd']).astype(int)\n",
    "    if 'GarageYrBlt' in all_data.columns:\n",
    "        all_data['GarageAge'] = all_data['YrSold'] - all_data['GarageYrBlt']\n",
    "        all_data['GarageAge'] = all_data['GarageAge'].fillna(0)\n",
    "\n",
    "if 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['HasBasement'] = (all_data['TotalBsmtSF'] > 0).astype(int)\n",
    "if 'GarageArea' in all_data.columns:\n",
    "    all_data['HasGarage'] = (all_data['GarageArea'] > 0).astype(int)\n",
    "if '2ndFlrSF' in all_data.columns:\n",
    "    all_data['Has2ndFloor'] = (all_data['2ndFlrSF'] > 0).astype(int)\n",
    "\n",
    "if 'OverallQual' in all_data.columns:\n",
    "    all_data['OverallQual2'] = all_data['OverallQual'] ** 2\n",
    "    if 'GrLivArea' in all_data.columns:\n",
    "        all_data['OverallQual_GrLivArea'] = all_data['OverallQual'] * all_data['GrLivArea']\n",
    "    if 'TotalBsmtSF' in all_data.columns:\n",
    "        all_data['OverallQual_TotalBsmtSF'] = all_data['OverallQual'] * all_data['TotalBsmtSF']\n",
    "    if 'GarageCars' in all_data.columns:\n",
    "        all_data['OverallQual_GarageCars'] = all_data['OverallQual'] * all_data['GarageCars']\n",
    "    if 'OverallCond' in all_data.columns:\n",
    "        all_data['OverallQual_OverallCond'] = all_data['OverallQual'] * all_data['OverallCond']\n",
    "\n",
    "if 'GrLivArea' in all_data.columns:\n",
    "    all_data['GrLivArea_log'] = np.log1p(all_data['GrLivArea'])\n",
    "    if 'TotalBathrooms' in all_data.columns:\n",
    "        all_data['AreaPerBath'] = all_data['GrLivArea'] / (all_data['TotalBathrooms'] + 0.1)\n",
    "    if 'TotRmsAbvGrd' in all_data.columns:\n",
    "        all_data['AreaPerRoom'] = all_data['GrLivArea'] / (all_data['TotRmsAbvGrd'] + 0.1)\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'GarageArea' in all_data.columns:\n",
    "    all_data['GarageAreaPerCar'] = all_data['GarageArea'] / (all_data['GarageCars'] + 0.1)\n",
    "\n",
    "if all(col in all_data.columns for col in ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'WoodDeckSF']):\n",
    "    all_data['TotalPorchSF'] = (all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + \n",
    "                                all_data['3SsnPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'OverallCond' in all_data.columns:\n",
    "    all_data['QualityScore'] = all_data['OverallQual'] * all_data['OverallCond']\n",
    "    all_data['QualityScore2'] = all_data['QualityScore'] ** 2\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'YearRemodAdd' in all_data.columns:\n",
    "    all_data['Remodeled'] = (all_data['YearBuilt'] != all_data['YearRemodAdd']).astype(int)\n",
    "    all_data['RemodelAge'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "\n",
    "if 'GrLivArea' in all_data.columns and 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['GrLivArea_TotalBsmtSF'] = all_data['GrLivArea'] * all_data['TotalBsmtSF']\n",
    "    all_data['GrLivArea_TotalBsmtSF_log'] = np.log1p(all_data['GrLivArea_TotalBsmtSF'])\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['OverallQual_GrLivArea_log'] = all_data['OverallQual'] * np.log1p(all_data['GrLivArea'])\n",
    "\n",
    "if 'TotalSF' in all_data.columns:\n",
    "    all_data['TotalSF2'] = all_data['TotalSF'] ** 2\n",
    "    all_data['TotalSF_sqrt'] = np.sqrt(all_data['TotalSF'])\n",
    "\n",
    "if 'GrLivArea' in all_data.columns:\n",
    "    all_data['GrLivArea_sqrt'] = np.sqrt(all_data['GrLivArea'])\n",
    "    all_data['GrLivArea_cbrt'] = np.power(all_data['GrLivArea'], 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "quality_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "\n",
    "for col in quality_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(quality_map).fillna(0).astype(int)\n",
    "\n",
    "exposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n",
    "if 'BsmtExposure' in all_data.columns:\n",
    "    all_data['BsmtExposure'] = all_data['BsmtExposure'].map(exposure_map).fillna(0).astype(int)\n",
    "\n",
    "finish_map = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0}\n",
    "for col in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(finish_map).fillna(0).astype(int)\n",
    "\n",
    "functional_map = {'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0}\n",
    "if 'Functional' in all_data.columns:\n",
    "    all_data['Functional'] = all_data['Functional'].map(functional_map).fillna(7).astype(int)\n",
    "\n",
    "if 'Neighborhood' in all_data.columns:\n",
    "    train_temp = all_data[:train_idx].copy()\n",
    "    train_temp['SalePrice'] = train_target\n",
    "    train_temp['SalePrice_log'] = np.log1p(train_target)\n",
    "    \n",
    "    neighborhood_stats = train_temp.groupby('Neighborhood')['SalePrice_log'].agg(['mean', 'std', 'count'])\n",
    "    global_mean = train_target.mean()\n",
    "    global_mean_log = np.log1p(global_mean)\n",
    "    \n",
    "    alpha = 5\n",
    "    neighborhood_encoded = (neighborhood_stats['mean'] * neighborhood_stats['count'] + global_mean_log * alpha) / (neighborhood_stats['count'] + alpha)\n",
    "    all_data['NeighborhoodEncoded'] = all_data['Neighborhood'].map(neighborhood_encoded.to_dict()).fillna(global_mean_log)\n",
    "    all_data['NeighborhoodEncoded_log'] = all_data['NeighborhoodEncoded']\n",
    "    \n",
    "    all_data['NeighborhoodStd'] = all_data['Neighborhood'].map(neighborhood_stats['std'].to_dict()).fillna(train_target.std())\n",
    "    all_data['NeighborhoodCount'] = all_data['Neighborhood'].map(neighborhood_stats['count'].to_dict()).fillna(0)\n",
    "\n",
    "if 'SaleType' in all_data.columns:\n",
    "    sale_type_map = {'New': 1, 'Con': 1, 'CWD': 0.8, 'ConLI': 0.7, 'WD': 0.5, \n",
    "                     'COD': 0.3, 'ConLw': 0.3, 'ConLD': 0.2, 'Oth': 0.1}\n",
    "    all_data['SaleTypeValue'] = all_data['SaleType'].map(sale_type_map).fillna(0.5)\n",
    "\n",
    "if 'SaleCondition' in all_data.columns:\n",
    "    sale_cond_map = {'Partial': 1.0, 'Normal': 0.8, 'Alloca': 0.7, 'Family': 0.6, \n",
    "                     'Abnorml': 0.4, 'AdjLand': 0.2}\n",
    "    all_data['SaleConditionValue'] = all_data['SaleCondition'].map(sale_cond_map).fillna(0.8)\n",
    "\n",
    "low_importance_features = ['Utilities', 'Street']\n",
    "for col in low_importance_features:\n",
    "    if col in all_data.columns:\n",
    "        all_data = all_data.drop(col, axis=1)\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data[col] = le.fit_transform(all_data[col].astype(str))\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8 outliers\n",
      "Selected 123 features (from 126)\n"
     ]
    }
   ],
   "source": [
    "train_processed = all_data[:train_idx].copy()\n",
    "test_processed = all_data[train_idx:].copy()\n",
    "\n",
    "train_processed = train_processed.drop('Id', axis=1)\n",
    "test_processed = test_processed.drop('Id', axis=1)\n",
    "\n",
    "outliers_grliv = train_processed[(train_processed['GrLivArea'] > 4000) & (y_train_log < 12.5)].index\n",
    "\n",
    "Q1 = train_processed['GrLivArea'].quantile(0.25)\n",
    "Q3 = train_processed['GrLivArea'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = train_processed[(train_processed['GrLivArea'] < (Q1 - 3 * IQR)) | \n",
    "                                (train_processed['GrLivArea'] > (Q3 + 3 * IQR))].index\n",
    "\n",
    "z_scores = np.abs(stats.zscore(train_processed[['GrLivArea', 'TotalBsmtSF']].fillna(0)))\n",
    "outliers_z = train_processed[(z_scores > 4).any(axis=1)].index\n",
    "\n",
    "outliers = list(set(list(outliers_grliv) + list(outliers_iqr) + list(outliers_z)))\n",
    "train_processed = train_processed.drop(outliers)\n",
    "y_train_log = y_train_log.drop(outliers)\n",
    "print(f\"Removed {len(outliers)} outliers\")\n",
    "\n",
    "mi_scores = mutual_info_regression(train_processed.fillna(0), y_train_log, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': train_processed.columns,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "rf_selector = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=n_jobs)\n",
    "rf_selector.fit(train_processed.fillna(0), y_train_log)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_processed.columns,\n",
    "    'rf_importance': rf_selector.feature_importances_\n",
    "})\n",
    "\n",
    "combined_importance = pd.merge(mi_df, feature_importance, on='feature')\n",
    "combined_importance['combined_score'] = (combined_importance['mi_score'] * 0.5 + \n",
    "                                      combined_importance['rf_importance'] * 0.5)\n",
    "combined_importance = combined_importance.sort_values('combined_score', ascending=False)\n",
    "\n",
    "important_features = combined_importance[combined_importance['combined_score'] > 0.0003]['feature'].tolist()\n",
    "train_processed = train_processed[important_features]\n",
    "test_processed = test_processed[important_features]\n",
    "print(f\"Selected {len(important_features)} features (from {len(combined_importance)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: 10 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "seeds = [42, 123, 456, 789, 2024, 999, 1337, 2023, 3141, 2718]\n",
    "all_rf_predictions = []\n",
    "\n",
    "for seed in seeds:\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=1500,\n",
    "        max_depth=28,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        random_state=seed,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    rf_model.fit(train_processed, y_train_log)\n",
    "    all_rf_predictions.append(np.expm1(rf_model.predict(test_processed)))\n",
    "\n",
    "rf_predictions = np.mean(all_rf_predictions, axis=0)\n",
    "print(f\"RF: {len(seeds)} seeds averaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: 10 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    all_xgb_predictions = []\n",
    "    for seed in seeds:\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=15000,\n",
    "            learning_rate=0.0025,\n",
    "            max_depth=7,\n",
    "            min_child_weight=2,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.85,\n",
    "            gamma=0.05,\n",
    "            reg_alpha=0.05,\n",
    "            reg_lambda=0.8,\n",
    "            random_state=seed,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        xgb_model.fit(train_processed, y_train_log, verbose=False)\n",
    "        all_xgb_predictions.append(np.expm1(xgb_model.predict(test_processed)))\n",
    "    \n",
    "    xgb_predictions = np.mean(all_xgb_predictions, axis=0)\n",
    "    print(f\"XGB: {len(seeds)} seeds averaged\")\n",
    "except ImportError:\n",
    "    xgb_predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB: 10 seeds averaged\n",
      "CAT: 10 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    all_lgb_predictions = []\n",
    "    for seed in seeds:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=15000,\n",
    "            learning_rate=0.0025,\n",
    "            max_depth=7,\n",
    "            num_leaves=127,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.85,\n",
    "            reg_alpha=0.05,\n",
    "            reg_lambda=0.8,\n",
    "            random_state=seed,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgb_model.fit(train_processed, y_train_log)\n",
    "        all_lgb_predictions.append(np.expm1(lgb_model.predict(test_processed)))\n",
    "    \n",
    "    lgb_predictions = np.mean(all_lgb_predictions, axis=0)\n",
    "    print(f\"LGB: {len(seeds)} seeds averaged\")\n",
    "except ImportError:\n",
    "    lgb_predictions = None\n",
    "\n",
    "cat_predictions = None\n",
    "try:\n",
    "    import catboost as cb\n",
    "    all_cat_predictions = []\n",
    "    for seed in seeds:\n",
    "        cat_model = cb.CatBoostRegressor(\n",
    "            iterations=15000,\n",
    "            learning_rate=0.0025,\n",
    "            depth=7,\n",
    "            l2_leaf_reg=3,\n",
    "            loss_function='RMSE',\n",
    "            eval_metric='RMSE',\n",
    "            random_seed=seed,\n",
    "            verbose=False,\n",
    "            thread_count=n_jobs\n",
    "        )\n",
    "        cat_model.fit(train_processed, y_train_log, verbose=False)\n",
    "        all_cat_predictions.append(np.expm1(cat_model.predict(test_processed)))\n",
    "    \n",
    "    cat_predictions = np.mean(all_cat_predictions, axis=0)\n",
    "    print(f\"CAT: {len(seeds)} seeds averaged\")\n",
    "except Exception as e:\n",
    "    print(f\"CatBoost error: {type(e).__name__}: {str(e)}\")\n",
    "    print(\"Skipping CatBoost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Added 1393 pseudo-labeled samples\n",
      "Iteration 2: Added 1377 pseudo-labeled samples\n"
     ]
    }
   ],
   "source": [
    "initial_predictions = (rf_predictions * 0.1 + \n",
    "                      (xgb_predictions * 0.3 if xgb_predictions is not None else rf_predictions * 0.3) +\n",
    "                      (lgb_predictions * 0.3 if lgb_predictions is not None else rf_predictions * 0.3) +\n",
    "                      (cat_predictions * 0.3 if cat_predictions is not None else rf_predictions * 0.3))\n",
    "\n",
    "test_confident = np.abs(initial_predictions - np.median(initial_predictions)) < (np.std(initial_predictions) * 2.5)\n",
    "confident_indices = np.where(test_confident)[0]\n",
    "\n",
    "if len(confident_indices) > 150:\n",
    "    for iteration in range(2):\n",
    "        pseudo_train = test_processed.iloc[confident_indices].copy()\n",
    "        pseudo_target = initial_predictions[confident_indices]\n",
    "        pseudo_target_log = np.log1p(pseudo_target)\n",
    "        \n",
    "        train_enhanced = pd.concat([train_processed, pseudo_train], ignore_index=True)\n",
    "        y_enhanced = pd.concat([pd.Series(y_train_log), pd.Series(pseudo_target_log)], ignore_index=True)\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}: Added {len(confident_indices)} pseudo-labeled samples\")\n",
    "        \n",
    "        rf_enhanced = RandomForestRegressor(n_estimators=1500, max_depth=28, min_samples_split=3,\n",
    "                                            min_samples_leaf=1, max_features='sqrt', random_state=42, n_jobs=n_jobs)\n",
    "        rf_enhanced.fit(train_enhanced, y_enhanced)\n",
    "        rf_predictions = np.expm1(rf_enhanced.predict(test_processed))\n",
    "        \n",
    "        if xgb_predictions is not None:\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                xgb_enhanced = xgb.XGBRegressor(n_estimators=15000, learning_rate=0.0025, max_depth=7,\n",
    "                                               min_child_weight=2, subsample=0.85, colsample_bytree=0.85,\n",
    "                                               gamma=0.05, reg_alpha=0.05, reg_lambda=0.8, random_state=42, n_jobs=n_jobs)\n",
    "                xgb_enhanced.fit(train_enhanced, y_enhanced, verbose=False)\n",
    "                xgb_predictions = np.expm1(xgb_enhanced.predict(test_processed))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if lgb_predictions is not None:\n",
    "            try:\n",
    "                import lightgbm as lgb\n",
    "                lgb_enhanced = lgb.LGBMRegressor(n_estimators=15000, learning_rate=0.0025, max_depth=7,\n",
    "                                                num_leaves=127, subsample=0.85, colsample_bytree=0.85,\n",
    "                                                reg_alpha=0.05, reg_lambda=0.8, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "                lgb_enhanced.fit(train_enhanced, y_enhanced)\n",
    "                lgb_predictions = np.expm1(lgb_enhanced.predict(test_processed))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if cat_predictions is not None:\n",
    "            try:\n",
    "                import catboost as cb\n",
    "                cat_enhanced = cb.CatBoostRegressor(iterations=15000, learning_rate=0.0025, depth=7,\n",
    "                                                   l2_leaf_reg=3, loss_function='RMSE', eval_metric='RMSE',\n",
    "                                                   random_seed=42, verbose=False, thread_count=n_jobs)\n",
    "                cat_enhanced.fit(train_enhanced, y_enhanced, verbose=False)\n",
    "                cat_predictions = np.expm1(cat_enhanced.predict(test_processed))\n",
    "            except Exception as e:\n",
    "                print(f\"CatBoost pseudo-labeling error: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        if iteration < 1:\n",
    "            updated_predictions = (rf_predictions * 0.1 + \n",
    "                                  (xgb_predictions * 0.3 if xgb_predictions is not None else rf_predictions * 0.3) +\n",
    "                                  (lgb_predictions * 0.3 if lgb_predictions is not None else rf_predictions * 0.3) +\n",
    "                                  (cat_predictions * 0.3 if cat_predictions is not None else rf_predictions * 0.3))\n",
    "            test_confident = np.abs(updated_predictions - np.median(updated_predictions)) < (np.std(updated_predictions) * 2.3)\n",
    "            confident_indices = np.where(test_confident)[0]\n",
    "            initial_predictions = updated_predictions\n",
    "else:\n",
    "    print(\"Not enough confident predictions for pseudo-labeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: 5 alphas averaged\n",
      "ElasticNet: 15 configs averaged\n",
      "GBR: 5 seeds averaged\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5682]\tvalid_0's l2: 0.0158028\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3864]\tvalid_0's l2: 0.0154766\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5750]\tvalid_0's l2: 0.0194279\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4217]\tvalid_0's l2: 0.0149986\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3387]\tvalid_0's l2: 0.0114392\n",
      "Non-linear stacking (XGB) OOF RMSE: 0.0996\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_processed)\n",
    "X_test_scaled = scaler.transform(test_processed)\n",
    "\n",
    "all_ridge_predictions = []\n",
    "for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    ridge_model = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge_model.fit(X_train_scaled, y_train_log)\n",
    "    all_ridge_predictions.append(np.expm1(ridge_model.predict(X_test_scaled)))\n",
    "\n",
    "all_elastic_predictions = []\n",
    "for alpha in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    for l1_ratio in [0.3, 0.5, 0.7]:\n",
    "        elastic_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "        elastic_model.fit(X_train_scaled, y_train_log)\n",
    "        all_elastic_predictions.append(np.expm1(elastic_model.predict(X_test_scaled)))\n",
    "\n",
    "all_gbr_predictions = []\n",
    "for seed in seeds[:5]:\n",
    "    gbr_model = GradientBoostingRegressor(n_estimators=500, learning_rate=0.01, max_depth=5,\n",
    "                                          random_state=seed, subsample=0.8)\n",
    "    gbr_model.fit(X_train_scaled, y_train_log)\n",
    "    all_gbr_predictions.append(np.expm1(gbr_model.predict(X_test_scaled)))\n",
    "\n",
    "ridge_predictions = np.mean(all_ridge_predictions, axis=0)\n",
    "elastic_predictions = np.mean(all_elastic_predictions, axis=0)\n",
    "gbr_predictions = np.mean(all_gbr_predictions, axis=0)\n",
    "print(f\"Ridge: {len(all_ridge_predictions)} alphas averaged\")\n",
    "print(f\"ElasticNet: {len(all_elastic_predictions)} configs averaged\")\n",
    "print(f\"GBR: {len(all_gbr_predictions)} seeds averaged\")\n",
    "\n",
    "oof_predictions = np.zeros((len(train_processed), 7))\n",
    "test_predictions = np.zeros((len(test_processed), 7))\n",
    "\n",
    "for fold, (train_idx_fold, val_idx_fold) in enumerate(kf.split(train_processed)):\n",
    "    X_train_fold = train_processed.iloc[train_idx_fold]\n",
    "    X_val_fold = train_processed.iloc[val_idx_fold]\n",
    "    y_train_fold = y_train_log.iloc[train_idx_fold]\n",
    "    y_val_fold = y_train_log.iloc[val_idx_fold]\n",
    "    \n",
    "    scaler_fold = RobustScaler()\n",
    "    X_train_scaled_fold = scaler_fold.fit_transform(X_train_fold)\n",
    "    X_val_scaled_fold = scaler_fold.transform(X_val_fold)\n",
    "    X_test_scaled_fold = scaler_fold.transform(test_processed)\n",
    "    \n",
    "    rf_fold = RandomForestRegressor(n_estimators=1500, max_depth=28, min_samples_split=3,\n",
    "                                    min_samples_leaf=1, max_features='sqrt', random_state=42, n_jobs=n_jobs)\n",
    "    rf_fold.fit(X_train_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 0] = rf_fold.predict(X_val_fold)\n",
    "    test_predictions[:, 0] += np.expm1(rf_fold.predict(test_processed)) / kf.n_splits\n",
    "    \n",
    "    if xgb_predictions is not None:\n",
    "        try:\n",
    "            import xgboost as xgb\n",
    "            xgb_fold = xgb.XGBRegressor(n_estimators=15000, learning_rate=0.0025, max_depth=7,\n",
    "                                        min_child_weight=2, subsample=0.85, colsample_bytree=0.85,\n",
    "                                        gamma=0.05, reg_alpha=0.05, reg_lambda=0.8, random_state=42, n_jobs=n_jobs)\n",
    "            xgb_fold.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)\n",
    "            oof_predictions[val_idx_fold, 1] = xgb_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 1] += np.expm1(xgb_fold.predict(test_processed)) / kf.n_splits\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if lgb_predictions is not None:\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            lgb_fold = lgb.LGBMRegressor(n_estimators=15000, learning_rate=0.0025, max_depth=7,\n",
    "                                        num_leaves=127, subsample=0.85, colsample_bytree=0.85,\n",
    "                                        reg_alpha=0.05, reg_lambda=0.8, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "            lgb_fold.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], \n",
    "                        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)])\n",
    "            oof_predictions[val_idx_fold, 2] = lgb_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 2] += np.expm1(lgb_fold.predict(test_processed)) / kf.n_splits\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    ridge_fold = Ridge(alpha=10.0, random_state=42)\n",
    "    ridge_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 3] = ridge_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 3] += np.expm1(ridge_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "    \n",
    "    elastic_fold = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42, max_iter=2000)\n",
    "    elastic_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 4] = elastic_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 4] += np.expm1(elastic_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "    \n",
    "    if cat_predictions is not None:\n",
    "        try:\n",
    "            import catboost as cb\n",
    "            cat_fold = cb.CatBoostRegressor(iterations=15000, learning_rate=0.0025, depth=7,\n",
    "                                           l2_leaf_reg=3, loss_function='RMSE', eval_metric='RMSE',\n",
    "                                           random_seed=42, verbose=False, thread_count=n_jobs)\n",
    "            cat_fold.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False)\n",
    "            oof_predictions[val_idx_fold, 5] = cat_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 5] += np.expm1(cat_fold.predict(test_processed)) / kf.n_splits\n",
    "        except Exception as e:\n",
    "            if fold == 0:\n",
    "                print(f\"CatBoost stacking error: {type(e).__name__}: {str(e)}\")\n",
    "    \n",
    "    gbr_fold = GradientBoostingRegressor(n_estimators=500, learning_rate=0.01, max_depth=5,\n",
    "                                        random_state=42, subsample=0.8)\n",
    "    gbr_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 6] = gbr_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 6] += np.expm1(gbr_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "\n",
    "valid_cols = [i for i in range(7) if oof_predictions[:, i].sum() != 0]\n",
    "oof_stack = oof_predictions[:, valid_cols]\n",
    "test_stack = test_predictions[:, valid_cols]\n",
    "\n",
    "test_stack_log = np.log1p(test_stack)\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    meta_model = xgb.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    meta_model.fit(oof_stack, y_train_log)\n",
    "    meta_rmse = np.sqrt(np.mean((meta_model.predict(oof_stack) - y_train_log) ** 2))\n",
    "    print(f\"Non-linear stacking (XGB) OOF RMSE: {meta_rmse:.4f}\")\n",
    "    final_predictions = np.expm1(meta_model.predict(test_stack_log))\n",
    "except:\n",
    "    def objective(weights):\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        blend = np.dot(oof_stack, weights)\n",
    "        return np.sqrt(np.mean((blend - y_train_log) ** 2))\n",
    "    \n",
    "    initial_weights = np.ones(len(valid_cols)) / len(valid_cols)\n",
    "    bounds = [(0, 1) for _ in range(len(valid_cols))]\n",
    "    result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, \n",
    "                      constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    optimal_weights = result.x / result.x.sum()\n",
    "    print(f\"Optimal weights: {optimal_weights}\")\n",
    "    print(f\"OOF RMSE with optimal weights: {result.fun:.4f}\")\n",
    "    final_predictions = np.expm1(np.dot(test_stack_log, optimal_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved: submission.csv\n",
      "Final predictions range: 49816.53 - 474660.44\n"
     ]
    }
   ],
   "source": [
    "predictions_for_geom = [rf_predictions]\n",
    "if xgb_predictions is not None:\n",
    "    predictions_for_geom.append(xgb_predictions)\n",
    "if lgb_predictions is not None:\n",
    "    predictions_for_geom.append(lgb_predictions)\n",
    "if cat_predictions is not None:\n",
    "    predictions_for_geom.append(cat_predictions)\n",
    "if 'gbr_predictions' in locals():\n",
    "    predictions_for_geom.append(gbr_predictions)\n",
    "\n",
    "geometric_mean = np.exp(np.mean([np.log(pred + 1) for pred in predictions_for_geom], axis=0)) - 1\n",
    "\n",
    "simple_weighted = (rf_predictions * 0.08 + \n",
    "                  (xgb_predictions * 0.25 if xgb_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (lgb_predictions * 0.25 if lgb_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (cat_predictions * 0.25 if cat_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (gbr_predictions * 0.17 if 'gbr_predictions' in locals() else rf_predictions * 0.17))\n",
    "\n",
    "median_pred = np.median([rf_predictions, \n",
    "                         xgb_predictions if xgb_predictions is not None else rf_predictions,\n",
    "                         lgb_predictions if lgb_predictions is not None else rf_predictions,\n",
    "                         cat_predictions if cat_predictions is not None else rf_predictions,\n",
    "                         gbr_predictions if 'gbr_predictions' in locals() else rf_predictions], axis=0)\n",
    "\n",
    "final_blend = 0.50 * final_predictions + 0.25 * geometric_mean + 0.15 * simple_weighted + 0.10 * median_pred\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'SalePrice': final_blend.clip(min=0)\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved: submission.csv\")\n",
    "print(f\"Final predictions range: {final_blend.min():.2f} - {final_blend.max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
