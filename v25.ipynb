{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - Advanced Regression Techniques\n",
    "## SCORE: .11941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 CPU cores (leaving 1 free)\n",
      "Train: (1460, 81), Test: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_jobs = max(1, os.cpu_count() - 1)\n",
    "print(f\"Using {n_jobs} CPU cores (leaving 1 free)\")\n",
    "\n",
    "data_dir = 'house-prices-advanced-regression-techniques'\n",
    "train = pd.read_csv(f'{data_dir}/train.csv')\n",
    "test = pd.read_csv(f'{data_dir}/test.csv')\n",
    "\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train['SalePrice'].copy()\n",
    "test_ids = test['Id'].copy()\n",
    "\n",
    "train_idx = len(train)\n",
    "all_data = pd.concat([train.drop('SalePrice', axis=1), test], ignore_index=True)\n",
    "\n",
    "y_train_log = np.log1p(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "\n",
    "none_cols = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "             'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', \n",
    "             'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "\n",
    "for col in none_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col].fillna('None', inplace=True)\n",
    "\n",
    "if 'LotFrontage' in all_data.columns:\n",
    "    all_data['LotFrontage'].fillna(all_data['LotFrontage'].median(), inplace=True)\n",
    "if 'MasVnrType' in all_data.columns:\n",
    "    all_data['MasVnrType'].fillna('None', inplace=True)\n",
    "if 'MasVnrArea' in all_data.columns:\n",
    "    all_data['MasVnrArea'].fillna(0, inplace=True)\n",
    "if 'Electrical' in all_data.columns:\n",
    "    all_data['Electrical'].fillna(all_data['Electrical'].mode()[0], inplace=True)\n",
    "if 'GarageYrBlt' in all_data.columns:\n",
    "    all_data['GarageYrBlt'].fillna(all_data['YearBuilt'], inplace=True)\n",
    "\n",
    "numerical_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if all_data[col].isnull().sum() > 0:\n",
    "        all_data[col].fillna(0, inplace=True)\n",
    "\n",
    "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if all_data[col].isnull().sum() > 0:\n",
    "        all_data[col].fillna(all_data[col].mode()[0], inplace=True)\n",
    "\n",
    "skewed_features = ['MiscVal', 'PoolArea', 'LotArea', '3SsnPorch', 'LowQualFinSF', \n",
    "                   'BsmtFinSF2', 'ScreenPorch', 'EnclosedPorch', 'MasVnrArea', \n",
    "                   'OpenPorchSF', 'LotFrontage', 'BsmtFinSF1', 'WoodDeckSF']\n",
    "for col in skewed_features:\n",
    "    if col in all_data.columns:\n",
    "        all_data[f'{col}_log'] = np.log1p(all_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(col in all_data.columns for col in ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']):\n",
    "    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "    all_data['TotalSF_log'] = np.log1p(all_data['TotalSF'])\n",
    "\n",
    "if all(col in all_data.columns for col in ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']):\n",
    "    all_data['TotalBathrooms'] = (all_data['FullBath'] + \n",
    "                                  all_data['HalfBath'] * 0.5 + \n",
    "                                  all_data['BsmtFullBath'] + \n",
    "                                  all_data['BsmtHalfBath'] * 0.5)\n",
    "\n",
    "if 'YrSold' in all_data.columns and 'YearBuilt' in all_data.columns:\n",
    "    all_data['HouseAge'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "    all_data['YearsSinceRemodel'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "    all_data['Remodeled'] = (all_data['YearBuilt'] != all_data['YearRemodAdd']).astype(int)\n",
    "    if 'GarageYrBlt' in all_data.columns:\n",
    "        all_data['GarageAge'] = all_data['YrSold'] - all_data['GarageYrBlt']\n",
    "        all_data['GarageAge'] = all_data['GarageAge'].fillna(0)\n",
    "\n",
    "if 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['HasBasement'] = (all_data['TotalBsmtSF'] > 0).astype(int)\n",
    "if 'GarageArea' in all_data.columns:\n",
    "    all_data['HasGarage'] = (all_data['GarageArea'] > 0).astype(int)\n",
    "if '2ndFlrSF' in all_data.columns:\n",
    "    all_data['Has2ndFloor'] = (all_data['2ndFlrSF'] > 0).astype(int)\n",
    "\n",
    "if 'OverallQual' in all_data.columns:\n",
    "    all_data['OverallQual2'] = all_data['OverallQual'] ** 2\n",
    "    if 'GrLivArea' in all_data.columns:\n",
    "        all_data['OverallQual_GrLivArea'] = all_data['OverallQual'] * all_data['GrLivArea']\n",
    "    if 'TotalBsmtSF' in all_data.columns:\n",
    "        all_data['OverallQual_TotalBsmtSF'] = all_data['OverallQual'] * all_data['TotalBsmtSF']\n",
    "    if 'GarageCars' in all_data.columns:\n",
    "        all_data['OverallQual_GarageCars'] = all_data['OverallQual'] * all_data['GarageCars']\n",
    "    if 'OverallCond' in all_data.columns:\n",
    "        all_data['OverallQual_OverallCond'] = all_data['OverallQual'] * all_data['OverallCond']\n",
    "\n",
    "if 'GrLivArea' in all_data.columns:\n",
    "    all_data['GrLivArea_log'] = np.log1p(all_data['GrLivArea'])\n",
    "    if 'TotalBathrooms' in all_data.columns:\n",
    "        all_data['AreaPerBath'] = all_data['GrLivArea'] / (all_data['TotalBathrooms'] + 0.1)\n",
    "    if 'TotRmsAbvGrd' in all_data.columns:\n",
    "        all_data['AreaPerRoom'] = all_data['GrLivArea'] / (all_data['TotRmsAbvGrd'] + 0.1)\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'GarageArea' in all_data.columns:\n",
    "    all_data['GarageAreaPerCar'] = all_data['GarageArea'] / (all_data['GarageCars'] + 0.1)\n",
    "\n",
    "if all(col in all_data.columns for col in ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'WoodDeckSF']):\n",
    "    all_data['TotalPorchSF'] = (all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + \n",
    "                                all_data['3SsnPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'OverallCond' in all_data.columns:\n",
    "    all_data['QualityScore'] = all_data['OverallQual'] * all_data['OverallCond']\n",
    "    all_data['QualityScore2'] = all_data['QualityScore'] ** 2\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'YearRemodAdd' in all_data.columns:\n",
    "    all_data['Remodeled'] = (all_data['YearBuilt'] != all_data['YearRemodAdd']).astype(int)\n",
    "    all_data['RemodelAge'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "\n",
    "if 'GrLivArea' in all_data.columns and 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['GrLivArea_TotalBsmtSF'] = all_data['GrLivArea'] * all_data['TotalBsmtSF']\n",
    "    all_data['GrLivArea_TotalBsmtSF_log'] = np.log1p(all_data['GrLivArea_TotalBsmtSF'])\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['OverallQual_GrLivArea_log'] = all_data['OverallQual'] * np.log1p(all_data['GrLivArea'])\n",
    "\n",
    "if 'TotalSF' in all_data.columns:\n",
    "    all_data['TotalSF2'] = all_data['TotalSF'] ** 2\n",
    "    all_data['TotalSF_sqrt'] = np.sqrt(all_data['TotalSF'])\n",
    "\n",
    "if 'GrLivArea' in all_data.columns:\n",
    "    all_data['GrLivArea_sqrt'] = np.sqrt(all_data['GrLivArea'])\n",
    "    all_data['GrLivArea_cbrt'] = np.power(all_data['GrLivArea'], 1/3)\n",
    "\n",
    "if 'LotArea' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['LotArea_GrLivArea_ratio'] = all_data['GrLivArea'] / (all_data['LotArea'] + 1)\n",
    "    all_data['LotArea_GrLivArea_diff'] = all_data['LotArea'] - all_data['GrLivArea']\n",
    "\n",
    "if 'BedroomAbvGrd' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['BedroomArea'] = all_data['GrLivArea'] / (all_data['BedroomAbvGrd'] + 1)\n",
    "\n",
    "if 'Fireplaces' in all_data.columns:\n",
    "    all_data['HasFireplace'] = (all_data['Fireplaces'] > 0).astype(int)\n",
    "\n",
    "if 'PoolArea' in all_data.columns:\n",
    "    all_data['HasPool'] = (all_data['PoolArea'] > 0).astype(int)\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'NeighborhoodEncoded' in all_data.columns:\n",
    "    all_data['OverallQual_Neighborhood'] = all_data['OverallQual'] * all_data['NeighborhoodEncoded']\n",
    "\n",
    "if 'TotalBathrooms' in all_data.columns and 'BedroomAbvGrd' in all_data.columns:\n",
    "    all_data['BathBedRatio'] = all_data['TotalBathrooms'] / (all_data['BedroomAbvGrd'] + 1)\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'TotalBathrooms' in all_data.columns:\n",
    "    all_data['OverallQual_TotalBathrooms'] = all_data['OverallQual'] * all_data['TotalBathrooms']\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'OverallQual' in all_data.columns:\n",
    "    all_data['YearBuilt_OverallQual'] = all_data['YearBuilt'] * all_data['OverallQual']\n",
    "\n",
    "if 'NeighborhoodEncoded' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['Neighborhood_GrLivArea'] = all_data['NeighborhoodEncoded'] * all_data['GrLivArea']\n",
    "\n",
    "if 'TotalSF' in all_data.columns and 'TotRmsAbvGrd' in all_data.columns:\n",
    "    all_data['TotalSF_per_Room'] = all_data['TotalSF'] / (all_data['TotRmsAbvGrd'] + 0.1)\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    all_data['GarageCars_TotalSF'] = all_data['GarageCars'] * all_data['TotalSF']\n",
    "\n",
    "# High-impact composite features from most important features\n",
    "if 'OverallQual' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    all_data['OverallQual_TotalSF'] = all_data['OverallQual'] * all_data['TotalSF']\n",
    "    all_data['OverallQual_TotalSF_log'] = all_data['OverallQual'] * np.log1p(all_data['TotalSF'])\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'NeighborhoodEncoded' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    all_data['QualityNeighborhoodSize'] = all_data['OverallQual'] * all_data['NeighborhoodEncoded'] * np.log1p(all_data['TotalSF'])\n",
    "\n",
    "if 'GrLivArea' in all_data.columns and 'TotalBathrooms' in all_data.columns and 'OverallQual' in all_data.columns:\n",
    "    all_data['QualityLivingBath'] = all_data['OverallQual'] * all_data['GrLivArea'] * (all_data['TotalBathrooms'] + 1)\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'OverallQual' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    all_data['ModernQualitySize'] = (all_data['YearBuilt'] - 1900) * all_data['OverallQual'] * np.log1p(all_data['TotalSF'])\n",
    "\n",
    "# Additional high-impact interactions based on domain knowledge\n",
    "if 'OverallQual' in all_data.columns and 'HouseAge' in all_data.columns:\n",
    "    # New high-quality houses vs old high-quality houses\n",
    "    all_data['QualityAgeInteraction'] = all_data['OverallQual'] * (1.0 / (np.abs(all_data['HouseAge']) + 1))\n",
    "    all_data['QualityAgeInteraction'] = np.clip(all_data['QualityAgeInteraction'], 0, 50)\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'OverallQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    # Garage capacity × Quality × Size\n",
    "    all_data['GarageQualitySize'] = all_data['GarageCars'] * all_data['OverallQual'] * np.log1p(all_data['GrLivArea'])\n",
    "\n",
    "if 'Fireplaces' in all_data.columns and 'OverallQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    # Fireplace value (number × quality × size)\n",
    "    all_data['FireplaceValue'] = all_data['Fireplaces'] * all_data['OverallQual'] * np.log1p(all_data['GrLivArea'] / 1000.0)\n",
    "    all_data['FireplaceValue'] = np.clip(all_data['FireplaceValue'], 0, 100)\n",
    "\n",
    "# MASSIVE INDICATORS: Garage is a huge value driver\n",
    "# Garage Premium Score (Garage is one of the strongest indicators)\n",
    "if 'GarageCars' in all_data.columns and 'GarageArea' in all_data.columns and 'OverallQual' in all_data.columns:\n",
    "    # Garage value = capacity × area × quality\n",
    "    all_data['GaragePremiumScore'] = all_data['GarageCars'] * np.log1p(all_data['GarageArea']) * all_data['OverallQual']\n",
    "    all_data['GaragePremiumScore'] = np.clip(all_data['GaragePremiumScore'], 0, 500)\n",
    "    \n",
    "    # Has premium garage (2+ cars, large area)\n",
    "    all_data['HasPremiumGarage'] = ((all_data['GarageCars'] >= 2) & (all_data['GarageArea'] > all_data['GarageArea'].median())).astype(int)\n",
    "\n",
    "# Modern Premium: Newer houses with high quality are worth significantly more\n",
    "if 'YearBuilt' in all_data.columns and 'OverallQual' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    # Normalize year (more recent = higher value)\n",
    "    year_norm = (all_data['YearBuilt'] - 1870) / (2010 - 1870)  # Approximate range\n",
    "    year_norm = np.clip(year_norm, 0, 1)\n",
    "    qual_norm = (all_data['OverallQual'] - 1) / 9.0\n",
    "    size_norm = (np.log1p(all_data['TotalSF']) - np.log1p(all_data['TotalSF']).min()) / (np.log1p(all_data['TotalSF']).max() - np.log1p(all_data['TotalSF']).min() + 1e-10)\n",
    "    \n",
    "    all_data['ModernPremiumScore'] = (year_norm * 0.3 + qual_norm * 0.4 + size_norm * 0.3) * 100\n",
    "    all_data['ModernPremiumScore'] = np.clip(all_data['ModernPremiumScore'], 0, 100)\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'NeighborhoodEncoded' in all_data.columns:\n",
    "    all_data['YearBuilt_Neighborhood'] = all_data['YearBuilt'] * all_data['NeighborhoodEncoded']\n",
    "\n",
    "if 'LotArea' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['LotUtilization'] = all_data['GrLivArea'] / (all_data['LotArea'] + 1)\n",
    "    all_data['LotUtilization'] = all_data['LotUtilization'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['LotUtilization'] = np.clip(all_data['LotUtilization'], 0, 10)\n",
    "\n",
    "if 'HouseAge' in all_data.columns and 'OverallQual' in all_data.columns:\n",
    "    all_data['AgeAdjustedQuality'] = all_data['OverallQual'] / (np.abs(all_data['HouseAge']) + 1)\n",
    "    all_data['AgeAdjustedQuality'] = all_data['AgeAdjustedQuality'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['AgeAdjustedQuality'] = np.clip(all_data['AgeAdjustedQuality'], 0, 10)\n",
    "\n",
    "if 'HouseAge' in all_data.columns and 'OverallCond' in all_data.columns:\n",
    "    all_data['AgeAdjustedCondition'] = all_data['OverallCond'] / (np.abs(all_data['HouseAge']) + 1)\n",
    "    all_data['AgeAdjustedCondition'] = all_data['AgeAdjustedCondition'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['AgeAdjustedCondition'] = np.clip(all_data['AgeAdjustedCondition'], 0, 10)\n",
    "\n",
    "if 'YearsSinceRemodel' in all_data.columns and 'Remodeled' in all_data.columns:\n",
    "    all_data['RemodelValue'] = all_data['Remodeled'] * (1.0 / (np.abs(all_data['YearsSinceRemodel']) + 1))\n",
    "    all_data['RemodelValue'] = all_data['RemodelValue'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['RemodelValue'] = np.clip(all_data['RemodelValue'], 0, 1)\n",
    "\n",
    "if 'TotalBsmtSF' in all_data.columns and 'BsmtFinSF1' in all_data.columns:\n",
    "    all_data['BasementFinishRatio'] = all_data['BsmtFinSF1'] / (all_data['TotalBsmtSF'] + 0.1)\n",
    "    all_data['BasementFinishRatio'] = all_data['BasementFinishRatio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['BasementFinishRatio'] = np.clip(all_data['BasementFinishRatio'], 0, 1)\n",
    "\n",
    "if 'TotalPorchSF' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['OutdoorLivingRatio'] = all_data['TotalPorchSF'] / (all_data['GrLivArea'] + 1)\n",
    "    all_data['OutdoorLivingRatio'] = all_data['OutdoorLivingRatio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['OutdoorLivingRatio'] = np.clip(all_data['OutdoorLivingRatio'], 0, 2)\n",
    "\n",
    "# Temporal feature encoding (seasonal patterns)\n",
    "if 'MoSold' in all_data.columns:\n",
    "    all_data['MoSold_sin'] = np.sin(2 * np.pi * all_data['MoSold'] / 12)\n",
    "    all_data['MoSold_cos'] = np.cos(2 * np.pi * all_data['MoSold'] / 12)\n",
    "    # Season indicators\n",
    "    all_data['SpringSale'] = ((all_data['MoSold'] >= 3) & (all_data['MoSold'] <= 5)).astype(int)\n",
    "    all_data['SummerSale'] = ((all_data['MoSold'] >= 6) & (all_data['MoSold'] <= 8)).astype(int)\n",
    "    all_data['FallSale'] = ((all_data['MoSold'] >= 9) & (all_data['MoSold'] <= 11)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "quality_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "\n",
    "for col in quality_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(quality_map).fillna(0).astype(int)\n",
    "\n",
    "exposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n",
    "if 'BsmtExposure' in all_data.columns:\n",
    "    all_data['BsmtExposure'] = all_data['BsmtExposure'].map(exposure_map).fillna(0).astype(int)\n",
    "\n",
    "finish_map = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0}\n",
    "for col in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(finish_map).fillna(0).astype(int)\n",
    "\n",
    "functional_map = {'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0}\n",
    "if 'Functional' in all_data.columns:\n",
    "    all_data['Functional'] = all_data['Functional'].map(functional_map).fillna(7).astype(int)\n",
    "\n",
    "# Better quality aggregation features (after quality mapping)\n",
    "if 'OverallQual' in all_data.columns:\n",
    "    # Quality consistency (how consistent is quality across features)\n",
    "    quality_cols_numeric = ['ExterQual', 'BsmtQual', 'KitchenQual', 'GarageQual']\n",
    "    available_quality_numeric = [col for col in quality_cols_numeric if col in all_data.columns and all_data[col].dtype in [np.int64, np.float64]]\n",
    "    if len(available_quality_numeric) > 1:\n",
    "        quality_std = all_data[available_quality_numeric].std(axis=1)\n",
    "        all_data['QualityConsistency'] = 1.0 / (quality_std + 0.1)  # Higher when more consistent\n",
    "        all_data['QualityConsistency'] = np.clip(all_data['QualityConsistency'], 0, 10)\n",
    "\n",
    "# MASSIVE INDICATOR: Kitchen Premium Score (Kitchen is often the most important room)\n",
    "# Created after quality mapping so KitchenQual is numeric\n",
    "if 'KitchenQual' in all_data.columns and 'GrLivArea' in all_data.columns and 'OverallQual' in all_data.columns:\n",
    "    # Kitchen value = quality × size × overall quality\n",
    "    all_data['KitchenPremiumScore'] = all_data['KitchenQual'] * np.log1p(all_data['GrLivArea']) * all_data['OverallQual']\n",
    "    all_data['KitchenPremiumScore'] = np.clip(all_data['KitchenPremiumScore'], 0, 500)\n",
    "    \n",
    "    # Premium kitchen indicator\n",
    "    all_data['HasPremiumKitchen'] = ((all_data['KitchenQual'] >= 4) & (all_data['GrLivArea'] > all_data['GrLivArea'].median())).astype(int)\n",
    "\n",
    "# Cross-validated target encoding to prevent data leakage\n",
    "kf_temp = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def cv_target_encode_feature(all_data, feature_name, train_idx, train_target, alpha=5, cv=kf_temp):\n",
    "    \"\"\"Cross-validated target encoding to prevent data leakage\"\"\"\n",
    "    train_data = all_data[:train_idx].copy()\n",
    "    test_data = all_data[train_idx:].copy()\n",
    "    train_data['SalePrice_log'] = np.log1p(train_target)\n",
    "    \n",
    "    # Initialize encoded columns\n",
    "    train_encoded = np.zeros(len(train_data))\n",
    "    test_encoded = np.zeros(len(test_data))\n",
    "    train_std = np.zeros(len(train_data))\n",
    "    test_std = np.zeros(len(test_data))\n",
    "    train_count = np.zeros(len(train_data))\n",
    "    test_count = np.zeros(len(test_data))\n",
    "    \n",
    "    global_mean_log = np.log1p(train_target.mean())\n",
    "    \n",
    "    # Cross-validated encoding for training data\n",
    "    for train_idx_fold, val_idx_fold in cv.split(train_data):\n",
    "        train_fold = train_data.iloc[train_idx_fold]\n",
    "        val_fold = train_data.iloc[val_idx_fold]\n",
    "        \n",
    "        # Calculate encoding on training fold\n",
    "        feature_stats = train_fold.groupby(feature_name)['SalePrice_log'].agg(['mean', 'std', 'count'])\n",
    "        feature_encoded = (feature_stats['mean'] * feature_stats['count'] + global_mean_log * alpha) / (feature_stats['count'] + alpha)\n",
    "        \n",
    "        # Apply to validation fold\n",
    "        train_encoded[val_idx_fold] = val_fold[feature_name].map(feature_encoded.to_dict()).fillna(global_mean_log).values\n",
    "        train_std[val_idx_fold] = val_fold[feature_name].map(feature_stats['std'].to_dict()).fillna(train_target.std()).values\n",
    "        train_count[val_idx_fold] = val_fold[feature_name].map(feature_stats['count'].to_dict()).fillna(0).values\n",
    "    \n",
    "    # Encode test data using full training set\n",
    "    feature_stats_full = train_data.groupby(feature_name)['SalePrice_log'].agg(['mean', 'std', 'count'])\n",
    "    feature_encoded_full = (feature_stats_full['mean'] * feature_stats_full['count'] + global_mean_log * alpha) / (feature_stats_full['count'] + alpha)\n",
    "    \n",
    "    test_encoded = test_data[feature_name].map(feature_encoded_full.to_dict()).fillna(global_mean_log).values\n",
    "    test_std = test_data[feature_name].map(feature_stats_full['std'].to_dict()).fillna(train_target.std()).values\n",
    "    test_count = test_data[feature_name].map(feature_stats_full['count'].to_dict()).fillna(0).values\n",
    "    \n",
    "    # Add to all_data\n",
    "    all_data[f'{feature_name}_Encoded'] = np.concatenate([train_encoded, test_encoded])\n",
    "    all_data[f'{feature_name}_Std'] = np.concatenate([train_std, test_std])\n",
    "    all_data[f'{feature_name}_Count'] = np.concatenate([train_count, test_count])\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "if 'Neighborhood' in all_data.columns:\n",
    "    all_data = cv_target_encode_feature(all_data, 'Neighborhood', train_idx, train_target, alpha=5)\n",
    "    all_data['NeighborhoodEncoded'] = all_data['Neighborhood_Encoded']\n",
    "    all_data['NeighborhoodEncoded_log'] = all_data['NeighborhoodEncoded']\n",
    "    all_data['NeighborhoodStd'] = all_data['Neighborhood_Std']\n",
    "    all_data['NeighborhoodCount'] = all_data['Neighborhood_Count']\n",
    "    \n",
    "    # Quality premium (OverallQual relative to neighborhood average) - after Neighborhood encoding\n",
    "    if 'OverallQual' in all_data.columns:\n",
    "        # Normalize neighborhood encoding to similar scale as OverallQual\n",
    "        neighborhood_norm = (all_data['NeighborhoodEncoded'] - all_data['NeighborhoodEncoded'].min()) / (all_data['NeighborhoodEncoded'].max() - all_data['NeighborhoodEncoded'].min() + 1e-10) * 10\n",
    "        all_data['QualityPremium'] = all_data['OverallQual'] - neighborhood_norm\n",
    "        all_data['QualityPremium'] = np.clip(all_data['QualityPremium'], -5, 5)\n",
    "    \n",
    "    # MASSIVE INDICATOR: Premium Value Score - combines the three strongest predictors\n",
    "    # This is Quality × Size × Neighborhood in a multiplicative way\n",
    "    if 'OverallQual' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "        # Normalize each component to 0-1 scale for stable multiplication\n",
    "        qual_norm = (all_data['OverallQual'] - 1) / 9.0  # 1-10 scale -> 0-1\n",
    "        neighborhood_norm = (all_data['NeighborhoodEncoded'] - all_data['NeighborhoodEncoded'].min()) / (all_data['NeighborhoodEncoded'].max() - all_data['NeighborhoodEncoded'].min() + 1e-10)\n",
    "        size_norm = (np.log1p(all_data['TotalSF']) - np.log1p(all_data['TotalSF']).min()) / (np.log1p(all_data['TotalSF']).max() - np.log1p(all_data['TotalSF']).min() + 1e-10)\n",
    "        \n",
    "        # Multiplicative combination (captures interactions)\n",
    "        all_data['PremiumValueScore'] = (qual_norm * 0.4 + neighborhood_norm * 0.35 + size_norm * 0.25) * 100\n",
    "        all_data['PremiumValueScore'] = np.clip(all_data['PremiumValueScore'], 0, 100)\n",
    "        \n",
    "        # Also create a multiplicative version (more aggressive)\n",
    "        all_data['PremiumValueScore_Mult'] = qual_norm * neighborhood_norm * size_norm * 1000\n",
    "        all_data['PremiumValueScore_Mult'] = np.clip(all_data['PremiumValueScore_Mult'], 0, 1000)\n",
    "        \n",
    "        # Premium indicator: Is this house significantly better than neighborhood average?\n",
    "        neighborhood_avg_qual = all_data.groupby('Neighborhood')['OverallQual'].transform('mean')\n",
    "        neighborhood_avg_size = all_data.groupby('Neighborhood')['TotalSF'].transform('mean')\n",
    "        all_data['QualityAboveNeighborhood'] = (all_data['OverallQual'] - neighborhood_avg_qual) / (neighborhood_avg_qual + 1e-10)\n",
    "        all_data['SizeAboveNeighborhood'] = (all_data['TotalSF'] - neighborhood_avg_size) / (neighborhood_avg_size + 1e-10)\n",
    "        all_data['QualityAboveNeighborhood'] = np.clip(all_data['QualityAboveNeighborhood'], -1, 2)\n",
    "        all_data['SizeAboveNeighborhood'] = np.clip(all_data['SizeAboveNeighborhood'], -1, 2)\n",
    "        \n",
    "        # Combined premium indicator\n",
    "        all_data['NeighborhoodPremium'] = (all_data['QualityAboveNeighborhood'] + all_data['SizeAboveNeighborhood']) / 2.0\n",
    "        all_data['NeighborhoodPremium'] = np.clip(all_data['NeighborhoodPremium'], -1, 2)\n",
    "\n",
    "# Cross-validated target encode other important categorical features\n",
    "for cat_feature in ['MSSubClass', 'MSZoning', 'LotShape', 'LandContour', 'LotConfig']:\n",
    "    if cat_feature in all_data.columns and all_data[cat_feature].dtype == 'object':\n",
    "        all_data = cv_target_encode_feature(all_data, cat_feature, train_idx, train_target, alpha=3)\n",
    "\n",
    "if 'SaleType' in all_data.columns:\n",
    "    sale_type_map = {'New': 1, 'Con': 1, 'CWD': 0.8, 'ConLI': 0.7, 'WD': 0.5, \n",
    "                     'COD': 0.3, 'ConLw': 0.3, 'ConLD': 0.2, 'Oth': 0.1}\n",
    "    all_data['SaleTypeValue'] = all_data['SaleType'].map(sale_type_map).fillna(0.5)\n",
    "\n",
    "if 'SaleCondition' in all_data.columns:\n",
    "    sale_cond_map = {'Partial': 1.0, 'Normal': 0.8, 'Alloca': 0.7, 'Family': 0.6, \n",
    "                     'Abnorml': 0.4, 'AdjLand': 0.2}\n",
    "    all_data['SaleConditionValue'] = all_data['SaleCondition'].map(sale_cond_map).fillna(0.8)\n",
    "\n",
    "low_importance_features = ['Utilities', 'Street']\n",
    "for col in low_importance_features:\n",
    "    if col in all_data.columns:\n",
    "        all_data = all_data.drop(col, axis=1)\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data[col] = le.fit_transform(all_data[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "if 'KitchenQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['KitchenQual_GrLivArea'] = all_data['KitchenQual'] * all_data['GrLivArea']\n",
    "\n",
    "if 'BsmtQual' in all_data.columns and 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['BsmtQual_TotalBsmtSF'] = all_data['BsmtQual'] * all_data['TotalBsmtSF']\n",
    "\n",
    "if 'ExterQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['ExterQual_GrLivArea'] = all_data['ExterQual'] * all_data['GrLivArea']\n",
    "\n",
    "if 'FireplaceQu' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['FireplaceQu_GrLivArea'] = all_data['FireplaceQu'] * all_data['GrLivArea']\n",
    "\n",
    "if 'GarageQual' in all_data.columns and 'GarageArea' in all_data.columns:\n",
    "    all_data['GarageQual_GarageArea'] = all_data['GarageQual'] * all_data['GarageArea']\n",
    "\n",
    "quality_features = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                    'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\n",
    "available_quality = [col for col in quality_features if col in all_data.columns]\n",
    "if len(available_quality) > 0:\n",
    "    all_data['TotalQualityScore'] = all_data[available_quality].sum(axis=1)\n",
    "\n",
    "if 'BsmtFinType1' in all_data.columns and 'BsmtFinSF1' in all_data.columns:\n",
    "    all_data['BasementFinishQuality'] = all_data['BsmtFinType1'] * all_data['BsmtFinSF1']\n",
    "\n",
    "if 'Functional' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['FunctionalValue'] = all_data['Functional'] * all_data['GrLivArea']\n",
    "\n",
    "if 'ExterQual' in all_data.columns and 'TotalPorchSF' in all_data.columns:\n",
    "    all_data['PorchQuality'] = all_data['ExterQual'] * all_data['TotalPorchSF']\n",
    "\n",
    "if 'SaleConditionValue' in all_data.columns and 'SaleTypeValue' in all_data.columns:\n",
    "    all_data['SaleValueFactor'] = all_data['SaleConditionValue'] * all_data['SaleTypeValue']\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'OverallCond' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['QualityConditionArea'] = (all_data['OverallQual'] + all_data['OverallCond']) * all_data['GrLivArea']\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'GarageArea' in all_data.columns:\n",
    "    all_data['GarageEfficiency'] = all_data['GarageArea'] / (all_data['GarageCars'] + 0.1)\n",
    "    all_data['GarageEfficiency'] = all_data['GarageEfficiency'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['GarageEfficiency'] = np.clip(all_data['GarageEfficiency'], 0, 1000)\n",
    "\n",
    "if 'TotRmsAbvGrd' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    all_data['RoomDensity'] = all_data['TotRmsAbvGrd'] / (all_data['TotalSF'] + 1)\n",
    "    all_data['RoomDensity'] = all_data['RoomDensity'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['RoomDensity'] = np.clip(all_data['RoomDensity'], 0, 1)\n",
    "\n",
    "if 'BedroomAbvGrd' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['BedroomDensity'] = all_data['BedroomAbvGrd'] / (all_data['GrLivArea'] + 1)\n",
    "    all_data['BedroomDensity'] = all_data['BedroomDensity'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['BedroomDensity'] = np.clip(all_data['BedroomDensity'], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8 extreme outliers, capped extreme values in remaining data\n",
      "Refined to 145 features based on model performance\n",
      "Selected 145 features (from 195, removed 29 redundant)\n"
     ]
    }
   ],
   "source": [
    "train_processed = all_data[:train_idx].copy()\n",
    "test_processed = all_data[train_idx:].copy()\n",
    "\n",
    "train_processed = train_processed.drop('Id', axis=1)\n",
    "test_processed = test_processed.drop('Id', axis=1)\n",
    "\n",
    "outliers_grliv = train_processed[(train_processed['GrLivArea'] > 4000) & (y_train_log < 12.5)].index\n",
    "\n",
    "Q1 = train_processed['GrLivArea'].quantile(0.25)\n",
    "Q3 = train_processed['GrLivArea'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = train_processed[(train_processed['GrLivArea'] < (Q1 - 3 * IQR)) | \n",
    "                                (train_processed['GrLivArea'] > (Q3 + 3 * IQR))].index\n",
    "\n",
    "z_scores = np.abs(stats.zscore(train_processed[['GrLivArea', 'TotalBsmtSF']].fillna(0)))\n",
    "outliers_z = train_processed[(z_scores > 4).any(axis=1)].index\n",
    "\n",
    "outliers = list(set(list(outliers_grliv) + list(outliers_iqr) + list(outliers_z)))\n",
    "# Remove extreme outliers\n",
    "train_processed = train_processed.drop(outliers)\n",
    "y_train_log = y_train_log.drop(outliers)\n",
    "\n",
    "# Cap extreme values in remaining data to retain information while reducing outlier impact\n",
    "if 'GrLivArea' in train_processed.columns:\n",
    "    Q1 = train_processed['GrLivArea'].quantile(0.01)\n",
    "    Q99 = train_processed['GrLivArea'].quantile(0.99)\n",
    "    train_processed['GrLivArea'] = train_processed['GrLivArea'].clip(Q1, Q99)\n",
    "    test_processed['GrLivArea'] = test_processed['GrLivArea'].clip(Q1, Q99)\n",
    "\n",
    "if 'TotalBsmtSF' in train_processed.columns:\n",
    "    Q1 = train_processed['TotalBsmtSF'].quantile(0.01)\n",
    "    Q99 = train_processed['TotalBsmtSF'].quantile(0.99)\n",
    "    train_processed['TotalBsmtSF'] = train_processed['TotalBsmtSF'].clip(Q1, Q99)\n",
    "    test_processed['TotalBsmtSF'] = test_processed['TotalBsmtSF'].clip(Q1, Q99)\n",
    "\n",
    "print(f\"Removed {len(outliers)} extreme outliers, capped extreme values in remaining data\")\n",
    "\n",
    "mi_scores = mutual_info_regression(train_processed.fillna(0), y_train_log, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': train_processed.columns,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "rf_selector = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=n_jobs)\n",
    "rf_selector.fit(train_processed.fillna(0), y_train_log)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_processed.columns,\n",
    "    'rf_importance': rf_selector.feature_importances_\n",
    "})\n",
    "\n",
    "combined_importance = pd.merge(mi_df, feature_importance, on='feature')\n",
    "combined_importance['combined_score'] = (combined_importance['mi_score'] * 0.5 + \n",
    "                                      combined_importance['rf_importance'] * 0.5)\n",
    "combined_importance = combined_importance.sort_values('combined_score', ascending=False)\n",
    "\n",
    "important_features = combined_importance[combined_importance['combined_score'] > 0.00025]['feature'].tolist()\n",
    "\n",
    "# Ensure premium/value score features are always included (they're massive indicators)\n",
    "premium_features = [f for f in train_processed.columns if any(keyword in f for keyword in ['Premium', 'ValueScore', 'NeighborhoodPremium', 'ModernPremium', 'GaragePremium', 'KitchenPremium'])]\n",
    "for pf in premium_features:\n",
    "    if pf not in important_features and pf in train_processed.columns:\n",
    "        important_features.append(pf)\n",
    "        # Add to combined_importance with high score to ensure it's kept\n",
    "        if pf not in combined_importance['feature'].values:\n",
    "            combined_importance = pd.concat([combined_importance, pd.DataFrame({\n",
    "                'feature': [pf],\n",
    "                'mi_score': [0.01],  # High score\n",
    "                'rf_importance': [0.01],\n",
    "                'combined_score': [0.01]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "train_temp = train_processed[important_features].copy()\n",
    "test_temp = test_processed[important_features].copy()\n",
    "\n",
    "# Correlation-based feature removal to reduce redundancy (more conservative threshold)\n",
    "corr_matrix = train_temp.corr().abs()\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find highly correlated pairs (>0.98 - only truly redundant features)\n",
    "high_corr_pairs = []\n",
    "for col in upper_triangle.columns:\n",
    "    for idx in upper_triangle.index:\n",
    "        if upper_triangle.loc[idx, col] > 0.98:\n",
    "            # Get importance scores for both features\n",
    "            feat1_score = combined_importance[combined_importance['feature'] == idx]['combined_score'].values\n",
    "            feat2_score = combined_importance[combined_importance['feature'] == col]['combined_score'].values\n",
    "            if len(feat1_score) > 0 and len(feat2_score) > 0:\n",
    "                if feat1_score[0] < feat2_score[0]:\n",
    "                    high_corr_pairs.append(idx)\n",
    "                else:\n",
    "                    high_corr_pairs.append(col)\n",
    "\n",
    "# Remove redundant features\n",
    "features_to_remove = list(set(high_corr_pairs))\n",
    "final_features = [f for f in important_features if f not in features_to_remove]\n",
    "\n",
    "# Additional feature selection: Use actual model performance to refine\n",
    "# Train a quick model to get feature importance from actual performance\n",
    "if len(final_features) > 100:\n",
    "    quick_rf = RandomForestRegressor(n_estimators=150, max_depth=12, random_state=42, n_jobs=n_jobs)\n",
    "    quick_rf.fit(train_temp[final_features], y_train_log)\n",
    "    feature_perf_importance = pd.DataFrame({\n",
    "        'feature': final_features,\n",
    "        'importance': quick_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Keep top features based on actual model performance (slightly more features)\n",
    "    top_n = min(145, len(final_features))\n",
    "    final_features = feature_perf_importance.head(top_n)['feature'].tolist()\n",
    "    print(f\"Refined to {len(final_features)} features based on model performance\")\n",
    "\n",
    "train_processed = train_temp[final_features]\n",
    "test_processed = test_temp[final_features]\n",
    "\n",
    "train_processed = train_processed.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "test_processed = test_processed.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "for col in train_processed.columns:\n",
    "    if train_processed[col].dtype in [np.float64, np.float32]:\n",
    "        train_processed[col] = np.clip(train_processed[col], -1e10, 1e10)\n",
    "        test_processed[col] = np.clip(test_processed[col], -1e10, 1e10)\n",
    "\n",
    "print(f\"Selected {len(final_features)} features (from {len(combined_importance)}, removed {len(features_to_remove)} redundant)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: 9 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "seeds = [42, 123, 456, 789, 2024, 999, 1337, 2023, 3141]\n",
    "all_rf_predictions = []\n",
    "\n",
    "for seed in seeds:\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=1200,\n",
    "        max_depth=25,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        random_state=seed,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    rf_model.fit(train_processed, y_train_log)\n",
    "    all_rf_predictions.append(np.expm1(rf_model.predict(test_processed)))\n",
    "\n",
    "rf_predictions = np.mean(all_rf_predictions, axis=0)\n",
    "print(f\"RF: {len(seeds)} seeds averaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: 9 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    all_xgb_predictions = []\n",
    "    for seed in seeds:\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=20000,\n",
    "            learning_rate=0.0018,\n",
    "            max_depth=6,\n",
    "            min_child_weight=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        xgb_model.fit(train_processed, y_train_log, verbose=False)\n",
    "        all_xgb_predictions.append(np.expm1(xgb_model.predict(test_processed)))\n",
    "    \n",
    "    xgb_predictions = np.mean(all_xgb_predictions, axis=0)\n",
    "    print(f\"XGB: {len(seeds)} seeds averaged\")\n",
    "except ImportError:\n",
    "    xgb_predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT: 9 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    all_lgb_predictions = []\n",
    "    for seed in seeds:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=20000,\n",
    "            learning_rate=0.0018,\n",
    "            max_depth=6,\n",
    "            num_leaves=63,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgb_model.fit(train_processed, y_train_log)\n",
    "        all_lgb_predictions.append(np.expm1(lgb_model.predict(test_processed)))\n",
    "    \n",
    "    lgb_predictions = np.mean(all_lgb_predictions, axis=0)\n",
    "    print(f\"LGB: {len(seeds)} seeds averaged\")\n",
    "except ImportError:\n",
    "    lgb_predictions = None\n",
    "\n",
    "cat_predictions = None\n",
    "try:\n",
    "    import catboost as cb\n",
    "    all_cat_predictions = []\n",
    "    for seed in seeds:\n",
    "        cat_model = cb.CatBoostRegressor(\n",
    "            iterations=20000,\n",
    "            learning_rate=0.0018,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=5,\n",
    "            loss_function='RMSE',\n",
    "            eval_metric='RMSE',\n",
    "            random_seed=seed,\n",
    "            verbose=False,\n",
    "            thread_count=n_jobs\n",
    "        )\n",
    "        cat_model.fit(train_processed, y_train_log, verbose=False)\n",
    "        all_cat_predictions.append(np.expm1(cat_model.predict(test_processed)))\n",
    "    \n",
    "    cat_predictions = np.mean(all_cat_predictions, axis=0)\n",
    "    print(f\"CAT: {len(seeds)} seeds averaged\")\n",
    "except Exception as e:\n",
    "    print(f\"CatBoost error: {type(e).__name__}: {str(e)}\")\n",
    "    print(\"Skipping CatBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Added 1052 pseudo-labeled samples\n",
      "Iteration 2: Added 1003 pseudo-labeled samples\n"
     ]
    }
   ],
   "source": [
    "all_initial_preds = []\n",
    "if rf_predictions is not None:\n",
    "    all_initial_preds.append(rf_predictions)\n",
    "if xgb_predictions is not None:\n",
    "    all_initial_preds.append(xgb_predictions)\n",
    "if lgb_predictions is not None:\n",
    "    all_initial_preds.append(lgb_predictions)\n",
    "if cat_predictions is not None:\n",
    "    all_initial_preds.append(cat_predictions)\n",
    "\n",
    "if len(all_initial_preds) > 0:\n",
    "    initial_predictions = np.mean(all_initial_preds, axis=0)\n",
    "    pred_variance = np.var(all_initial_preds, axis=0)\n",
    "    pred_std = np.std(all_initial_preds, axis=0)\n",
    "    \n",
    "    ensemble_agreement = pred_std < (np.median(pred_std) * 1.6)\n",
    "    median_distance = np.abs(initial_predictions - np.median(initial_predictions)) < (np.std(initial_predictions) * 2.3)\n",
    "    test_confident = ensemble_agreement & median_distance\n",
    "    confident_indices = np.where(test_confident)[0]\n",
    "    \n",
    "    confidence_weights = 1.0 / (pred_std + 0.01)\n",
    "    confidence_weights = confidence_weights / confidence_weights.max()\n",
    "else:\n",
    "    initial_predictions = rf_predictions\n",
    "    test_confident = np.abs(initial_predictions - np.median(initial_predictions)) < (np.std(initial_predictions) * 2.2)\n",
    "    confident_indices = np.where(test_confident)[0]\n",
    "    confidence_weights = np.ones(len(initial_predictions))\n",
    "\n",
    "if len(confident_indices) > 200:\n",
    "    for iteration in range(2):\n",
    "        pseudo_train = test_processed.iloc[confident_indices].copy()\n",
    "        pseudo_target = initial_predictions[confident_indices]\n",
    "        pseudo_target_log = np.log1p(pseudo_target)\n",
    "        pseudo_weights = confidence_weights[confident_indices]\n",
    "        \n",
    "        train_enhanced = pd.concat([train_processed, pseudo_train], ignore_index=True)\n",
    "        y_enhanced = pd.concat([pd.Series(y_train_log), pd.Series(pseudo_target_log)], ignore_index=True)\n",
    "        sample_weights = pd.concat([pd.Series(np.ones(len(y_train_log))), pd.Series(pseudo_weights)], ignore_index=True)\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}: Added {len(confident_indices)} pseudo-labeled samples\")\n",
    "        \n",
    "        rf_enhanced = RandomForestRegressor(n_estimators=1200, max_depth=25, min_samples_split=5,\n",
    "                                            min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=n_jobs)\n",
    "        rf_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights)\n",
    "        rf_predictions = np.expm1(rf_enhanced.predict(test_processed))\n",
    "        \n",
    "        if xgb_predictions is not None:\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                xgb_enhanced = xgb.XGBRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                               min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                               gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs)\n",
    "                xgb_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights, verbose=False)\n",
    "                xgb_predictions = np.expm1(xgb_enhanced.predict(test_processed))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if lgb_predictions is not None:\n",
    "            try:\n",
    "                import lightgbm as lgb\n",
    "                lgb_enhanced = lgb.LGBMRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                                num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "                lgb_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights)\n",
    "                lgb_predictions = np.expm1(lgb_enhanced.predict(test_processed))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if cat_predictions is not None:\n",
    "            try:\n",
    "                import catboost as cb\n",
    "                cat_enhanced = cb.CatBoostRegressor(iterations=20000, learning_rate=0.0018, depth=6,\n",
    "                                                   l2_leaf_reg=5, loss_function='RMSE', eval_metric='RMSE',\n",
    "                                                   random_seed=42, verbose=False, thread_count=n_jobs)\n",
    "                cat_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights, verbose=False)\n",
    "                cat_predictions = np.expm1(cat_enhanced.predict(test_processed))\n",
    "            except Exception as e:\n",
    "                print(f\"CatBoost pseudo-labeling error: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        if iteration < 2:\n",
    "            all_updated_preds = []\n",
    "            if rf_predictions is not None:\n",
    "                all_updated_preds.append(rf_predictions)\n",
    "            if xgb_predictions is not None:\n",
    "                all_updated_preds.append(xgb_predictions)\n",
    "            if lgb_predictions is not None:\n",
    "                all_updated_preds.append(lgb_predictions)\n",
    "            if cat_predictions is not None:\n",
    "                all_updated_preds.append(cat_predictions)\n",
    "            \n",
    "            if len(all_updated_preds) > 0:\n",
    "                updated_predictions = np.mean(all_updated_preds, axis=0)\n",
    "                pred_std = np.std(all_updated_preds, axis=0)\n",
    "                \n",
    "                ensemble_agreement = pred_std < (np.median(pred_std) * (1.5 - iteration * 0.1))\n",
    "                median_distance = np.abs(updated_predictions - np.median(updated_predictions)) < (np.std(updated_predictions) * (2.2 - iteration * 0.1))\n",
    "                test_confident = ensemble_agreement & median_distance\n",
    "                confident_indices = np.where(test_confident)[0]\n",
    "                initial_predictions = updated_predictions\n",
    "                \n",
    "                confidence_weights = 1.0 / (pred_std + 0.01)\n",
    "                confidence_weights = confidence_weights / confidence_weights.max()\n",
    "else:\n",
    "    print(\"Not enough confident predictions for pseudo-labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: 5 alphas averaged\n",
      "ElasticNet: 15 configs averaged\n",
      "GBR: 7 seeds averaged\n",
      "Meta XGB OOF RMSE: 0.0990\n",
      "Meta LGB error: No module named 'lightgbm'\n",
      "Meta Ridge OOF RMSE: 0.1104\n",
      "Meta models used: ['xgb', 'ridge']\n",
      "Optimal meta-blend weights: [1. 0.]\n",
      "Meta-blend OOF RMSE: 0.0990\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_processed)\n",
    "X_test_scaled = scaler.transform(test_processed)\n",
    "\n",
    "all_ridge_predictions = []\n",
    "for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    ridge_model = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge_model.fit(X_train_scaled, y_train_log)\n",
    "    all_ridge_predictions.append(np.expm1(ridge_model.predict(X_test_scaled)))\n",
    "\n",
    "all_elastic_predictions = []\n",
    "for alpha in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    for l1_ratio in [0.3, 0.5, 0.7]:\n",
    "        elastic_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "        elastic_model.fit(X_train_scaled, y_train_log)\n",
    "        all_elastic_predictions.append(np.expm1(elastic_model.predict(X_test_scaled)))\n",
    "\n",
    "all_gbr_predictions = []\n",
    "for seed in seeds[:7]:\n",
    "    gbr_model = GradientBoostingRegressor(n_estimators=600, learning_rate=0.01, max_depth=5,\n",
    "                                          random_state=seed, subsample=0.8)\n",
    "    gbr_model.fit(X_train_scaled, y_train_log)\n",
    "    all_gbr_predictions.append(np.expm1(gbr_model.predict(X_test_scaled)))\n",
    "\n",
    "ridge_predictions = np.mean(all_ridge_predictions, axis=0)\n",
    "elastic_predictions = np.mean(all_elastic_predictions, axis=0)\n",
    "gbr_predictions = np.mean(all_gbr_predictions, axis=0)\n",
    "print(f\"Ridge: {len(all_ridge_predictions)} alphas averaged\")\n",
    "print(f\"ElasticNet: {len(all_elastic_predictions)} configs averaged\")\n",
    "print(f\"GBR: {len(all_gbr_predictions)} seeds averaged\")\n",
    "\n",
    "oof_predictions = np.zeros((len(train_processed), 7))\n",
    "test_predictions = np.zeros((len(test_processed), 7))\n",
    "\n",
    "for fold, (train_idx_fold, val_idx_fold) in enumerate(kf.split(train_processed)):\n",
    "    X_train_fold = train_processed.iloc[train_idx_fold]\n",
    "    X_val_fold = train_processed.iloc[val_idx_fold]\n",
    "    y_train_fold = y_train_log.iloc[train_idx_fold]\n",
    "    y_val_fold = y_train_log.iloc[val_idx_fold]\n",
    "    \n",
    "    scaler_fold = RobustScaler()\n",
    "    X_train_scaled_fold = scaler_fold.fit_transform(X_train_fold)\n",
    "    X_val_scaled_fold = scaler_fold.transform(X_val_fold)\n",
    "    X_test_scaled_fold = scaler_fold.transform(test_processed)\n",
    "    \n",
    "    rf_fold = RandomForestRegressor(n_estimators=1200, max_depth=25, min_samples_split=5,\n",
    "                                    min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=n_jobs)\n",
    "    rf_fold.fit(X_train_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 0] = rf_fold.predict(X_val_fold)\n",
    "    test_predictions[:, 0] += np.expm1(rf_fold.predict(test_processed)) / kf.n_splits\n",
    "    \n",
    "    if xgb_predictions is not None:\n",
    "        try:\n",
    "            import xgboost as xgb\n",
    "            xgb_fold = xgb.XGBRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs)\n",
    "            xgb_fold.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)\n",
    "            oof_predictions[val_idx_fold, 1] = xgb_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 1] += np.expm1(xgb_fold.predict(test_processed)) / kf.n_splits\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if lgb_predictions is not None:\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            lgb_fold = lgb.LGBMRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                        num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "            lgb_fold.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], \n",
    "                        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)])\n",
    "            oof_predictions[val_idx_fold, 2] = lgb_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 2] += np.expm1(lgb_fold.predict(test_processed)) / kf.n_splits\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    ridge_fold = Ridge(alpha=10.0, random_state=42)\n",
    "    ridge_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 3] = ridge_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 3] += np.expm1(ridge_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "    \n",
    "    elastic_fold = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42, max_iter=2000)\n",
    "    elastic_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 4] = elastic_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 4] += np.expm1(elastic_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "    \n",
    "    if cat_predictions is not None:\n",
    "        try:\n",
    "            import catboost as cb\n",
    "            cat_fold = cb.CatBoostRegressor(iterations=20000, learning_rate=0.0018, depth=6,\n",
    "                                           l2_leaf_reg=5, loss_function='RMSE', eval_metric='RMSE',\n",
    "                                           random_seed=42, verbose=False, thread_count=n_jobs)\n",
    "            cat_fold.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False)\n",
    "            oof_predictions[val_idx_fold, 5] = cat_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 5] += np.expm1(cat_fold.predict(test_processed)) / kf.n_splits\n",
    "        except Exception as e:\n",
    "            if fold == 0:\n",
    "                print(f\"CatBoost stacking error: {type(e).__name__}: {str(e)}\")\n",
    "    \n",
    "    gbr_fold = GradientBoostingRegressor(n_estimators=600, learning_rate=0.01, max_depth=5,\n",
    "                                        random_state=42, subsample=0.8)\n",
    "    gbr_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 6] = gbr_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 6] += np.expm1(gbr_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "\n",
    "valid_cols = [i for i in range(7) if oof_predictions[:, i].sum() != 0]\n",
    "oof_stack = oof_predictions[:, valid_cols]\n",
    "test_stack = test_predictions[:, valid_cols]\n",
    "\n",
    "# Clean test_stack before log transformation\n",
    "test_stack = np.clip(test_stack, 0, 1e10)  # Clip extreme values\n",
    "test_stack = np.nan_to_num(test_stack, nan=0.0, posinf=1e10, neginf=0.0)\n",
    "test_stack_log = np.log1p(test_stack)\n",
    "# Clean test_stack_log after transformation\n",
    "test_stack_log = np.clip(test_stack_log, -50, 50)  # Reasonable log space bounds\n",
    "test_stack_log = np.nan_to_num(test_stack_log, nan=0.0, posinf=50.0, neginf=-50.0)\n",
    "\n",
    "# Multi-level stacking: Train multiple meta-models and blend them\n",
    "meta_predictions = []\n",
    "meta_oof_predictions_list = []\n",
    "meta_model_names = []\n",
    "\n",
    "# Meta-model 1: XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    meta_xgb = xgb.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    meta_xgb.fit(oof_stack, y_train_log)\n",
    "    meta_xgb_oof = meta_xgb.predict(oof_stack)\n",
    "    meta_xgb_rmse = np.sqrt(np.mean((meta_xgb_oof - y_train_log) ** 2))\n",
    "    meta_oof_predictions_list.append(meta_xgb_oof)\n",
    "    meta_xgb_test_pred = meta_xgb.predict(test_stack_log)\n",
    "    # Ensure predictions are in valid range\n",
    "    meta_xgb_test_pred = np.clip(meta_xgb_test_pred, -20, 20)\n",
    "    meta_predictions.append(np.expm1(meta_xgb_test_pred))\n",
    "    meta_model_names.append('xgb')\n",
    "    print(f\"Meta XGB OOF RMSE: {meta_xgb_rmse:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Meta XGB error: {e}\")\n",
    "\n",
    "# Meta-model 2: LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    meta_lgb = lgb.LGBMRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        num_leaves=15,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=-1\n",
    "    )\n",
    "    meta_lgb.fit(oof_stack, y_train_log)\n",
    "    meta_lgb_oof = meta_lgb.predict(oof_stack)\n",
    "    meta_lgb_rmse = np.sqrt(np.mean((meta_lgb_oof - y_train_log) ** 2))\n",
    "    meta_oof_predictions_list.append(meta_lgb_oof)\n",
    "    meta_lgb_test_pred = meta_lgb.predict(test_stack_log)\n",
    "    # Ensure predictions are in valid range\n",
    "    meta_lgb_test_pred = np.clip(meta_lgb_test_pred, -20, 20)\n",
    "    meta_predictions.append(np.expm1(meta_lgb_test_pred))\n",
    "    meta_model_names.append('lgb')\n",
    "    print(f\"Meta LGB OOF RMSE: {meta_lgb_rmse:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Meta LGB error: {e}\")\n",
    "\n",
    "# Meta-model 3: Ridge (for linear combination)\n",
    "meta_ridge = Ridge(alpha=10.0, random_state=42)\n",
    "meta_ridge.fit(oof_stack, y_train_log)\n",
    "meta_ridge_oof = meta_ridge.predict(oof_stack)\n",
    "meta_ridge_rmse = np.sqrt(np.mean((meta_ridge_oof - y_train_log) ** 2))\n",
    "meta_oof_predictions_list.append(meta_ridge_oof)\n",
    "meta_ridge_test_pred = meta_ridge.predict(test_stack_log)\n",
    "# Ensure predictions are in valid range\n",
    "meta_ridge_test_pred = np.clip(meta_ridge_test_pred, -20, 20)\n",
    "meta_predictions.append(np.expm1(meta_ridge_test_pred))\n",
    "meta_model_names.append('ridge')\n",
    "print(f\"Meta Ridge OOF RMSE: {meta_ridge_rmse:.4f}\")\n",
    "\n",
    "# Blend meta-model predictions using OOF performance\n",
    "if len(meta_predictions) > 0:\n",
    "    # Build arrays from successful models only\n",
    "    meta_oof_array = np.column_stack(meta_oof_predictions_list)\n",
    "    meta_test_array = np.column_stack(meta_predictions)\n",
    "    \n",
    "    # Optimize meta-blend weights\n",
    "    n_meta_models = len(meta_predictions)\n",
    "    def meta_blend_objective(weights):\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        blend = np.dot(meta_oof_array, weights)\n",
    "        return np.sqrt(np.mean((blend - y_train_log) ** 2))\n",
    "    \n",
    "    initial_meta_weights = np.ones(n_meta_models) / n_meta_models\n",
    "    bounds = [(0, 1) for _ in range(n_meta_models)]\n",
    "    result_meta = minimize(meta_blend_objective, initial_meta_weights, method='SLSQP', bounds=bounds,\n",
    "                          constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    optimal_meta_weights = result_meta.x / result_meta.x.sum()\n",
    "    print(f\"Meta models used: {meta_model_names}\")\n",
    "    print(f\"Optimal meta-blend weights: {optimal_meta_weights}\")\n",
    "    print(f\"Meta-blend OOF RMSE: {result_meta.fun:.4f}\")\n",
    "    \n",
    "    final_predictions = np.dot(meta_test_array, optimal_meta_weights)\n",
    "else:\n",
    "    # Fallback to simple weighted average\n",
    "    def objective(weights):\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        blend = np.dot(oof_stack, weights)\n",
    "        return np.sqrt(np.mean((blend - y_train_log) ** 2))\n",
    "    \n",
    "    initial_weights = np.ones(len(valid_cols)) / len(valid_cols)\n",
    "    bounds = [(0, 1) for _ in range(len(valid_cols))]\n",
    "    result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, \n",
    "                      constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    optimal_weights = result.x / result.x.sum()\n",
    "    print(f\"Optimal weights: {optimal_weights}\")\n",
    "    print(f\"OOF RMSE with optimal weights: {result.fun:.4f}\")\n",
    "    final_predictions = np.expm1(np.dot(test_stack_log, optimal_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal blend weights: [1. 0. 0. 0.]\n",
      "OOF RMSE with optimal blend: 0.1123\n",
      "Submission saved: submission.csv\n",
      "Final predictions range: 53375.05 - 467788.06\n",
      "\n",
      "🎉 Model training complete! Submission ready.\n"
     ]
    }
   ],
   "source": [
    "predictions_for_geom = [rf_predictions]\n",
    "if xgb_predictions is not None:\n",
    "    predictions_for_geom.append(xgb_predictions)\n",
    "if lgb_predictions is not None:\n",
    "    predictions_for_geom.append(lgb_predictions)\n",
    "if cat_predictions is not None:\n",
    "    predictions_for_geom.append(cat_predictions)\n",
    "if 'gbr_predictions' in locals():\n",
    "    predictions_for_geom.append(gbr_predictions)\n",
    "\n",
    "geometric_mean = np.exp(np.mean([np.log(pred + 1) for pred in predictions_for_geom], axis=0)) - 1\n",
    "\n",
    "simple_weighted = (rf_predictions * 0.08 + \n",
    "                  (xgb_predictions * 0.25 if xgb_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (lgb_predictions * 0.25 if lgb_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (cat_predictions * 0.25 if cat_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (gbr_predictions * 0.17 if 'gbr_predictions' in locals() else rf_predictions * 0.17))\n",
    "\n",
    "median_pred = np.median([rf_predictions, \n",
    "                         xgb_predictions if xgb_predictions is not None else rf_predictions,\n",
    "                         lgb_predictions if lgb_predictions is not None else rf_predictions,\n",
    "                         cat_predictions if cat_predictions is not None else rf_predictions,\n",
    "                         gbr_predictions if 'gbr_predictions' in locals() else rf_predictions], axis=0)\n",
    "\n",
    "oof_base_predictions = {\n",
    "    'stacked': oof_stack.mean(axis=1) if len(valid_cols) > 0 else np.zeros(len(y_train_log)),\n",
    "    'geometric': np.zeros(len(y_train_log)),\n",
    "    'weighted': np.zeros(len(y_train_log)),\n",
    "    'median': np.zeros(len(y_train_log))\n",
    "}\n",
    "\n",
    "for fold, (train_idx_fold, val_idx_fold) in enumerate(kf.split(train_processed)):\n",
    "    fold_geom_preds = []\n",
    "    fold_weighted_preds = []\n",
    "    fold_median_preds = []\n",
    "    \n",
    "    rf_fold_pred = np.expm1(oof_predictions[val_idx_fold, 0])\n",
    "    fold_geom_preds.append(rf_fold_pred)\n",
    "    fold_weighted_preds.append(rf_fold_pred * 0.08)\n",
    "    fold_median_preds.append(rf_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 1].sum() > 0:\n",
    "        xgb_fold_pred = np.expm1(oof_predictions[val_idx_fold, 1])\n",
    "        fold_geom_preds.append(xgb_fold_pred)\n",
    "        fold_weighted_preds.append(xgb_fold_pred * 0.25)\n",
    "        fold_median_preds.append(xgb_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 2].sum() > 0:\n",
    "        lgb_fold_pred = np.expm1(oof_predictions[val_idx_fold, 2])\n",
    "        fold_geom_preds.append(lgb_fold_pred)\n",
    "        fold_weighted_preds.append(lgb_fold_pred * 0.25)\n",
    "        fold_median_preds.append(lgb_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 5].sum() > 0:\n",
    "        cat_fold_pred = np.expm1(oof_predictions[val_idx_fold, 5])\n",
    "        fold_geom_preds.append(cat_fold_pred)\n",
    "        fold_weighted_preds.append(cat_fold_pred * 0.25)\n",
    "        fold_median_preds.append(cat_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 6].sum() > 0:\n",
    "        gbr_fold_pred = np.expm1(oof_predictions[val_idx_fold, 6])\n",
    "        fold_geom_preds.append(gbr_fold_pred)\n",
    "        fold_weighted_preds.append(gbr_fold_pred * 0.17)\n",
    "        fold_median_preds.append(gbr_fold_pred)\n",
    "    \n",
    "    if fold_geom_preds:\n",
    "        oof_base_predictions['geometric'][val_idx_fold] = np.exp(np.mean([np.log(pred + 1) for pred in fold_geom_preds], axis=0)) - 1\n",
    "        oof_base_predictions['weighted'][val_idx_fold] = np.sum(fold_weighted_preds, axis=0)\n",
    "        oof_base_predictions['median'][val_idx_fold] = np.median(fold_median_preds, axis=0)\n",
    "\n",
    "oof_base_predictions['stacked'] = np.expm1(oof_stack.mean(axis=1)) if len(valid_cols) > 0 else np.zeros(len(y_train_log))\n",
    "\n",
    "blend_oof_array = np.column_stack([\n",
    "    oof_base_predictions['stacked'],\n",
    "    oof_base_predictions['geometric'],\n",
    "    oof_base_predictions['weighted'],\n",
    "    oof_base_predictions['median']\n",
    "])\n",
    "\n",
    "def blend_objective(weights):\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    blend = np.dot(blend_oof_array, weights)\n",
    "    return np.sqrt(np.mean((np.log1p(blend) - y_train_log) ** 2))\n",
    "\n",
    "initial_blend_weights = np.array([0.50, 0.25, 0.15, 0.10])\n",
    "bounds = [(0, 1) for _ in range(4)]\n",
    "result_blend = minimize(blend_objective, initial_blend_weights, method='SLSQP', bounds=bounds,\n",
    "                        constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "optimal_blend_weights = result_blend.x / result_blend.x.sum()\n",
    "print(f\"Optimal blend weights: {optimal_blend_weights}\")\n",
    "print(f\"OOF RMSE with optimal blend: {result_blend.fun:.4f}\")\n",
    "\n",
    "final_blend = (optimal_blend_weights[0] * final_predictions +\n",
    "               optimal_blend_weights[1] * geometric_mean +\n",
    "               optimal_blend_weights[2] * simple_weighted +\n",
    "               optimal_blend_weights[3] * median_pred)\n",
    "\n",
    "max_price = np.expm1(y_train_log.max()) * 1.5\n",
    "min_price = np.expm1(y_train_log.min()) * 0.5\n",
    "\n",
    "final_blend = np.clip(final_blend, min_price, max_price)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'SalePrice': final_blend\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved: submission.csv\")\n",
    "print(f\"Final predictions range: {final_blend.min():.2f} - {final_blend.max():.2f}\")\n",
    "\n",
    "# Play completion sound\n",
    "try:\n",
    "    import winsound\n",
    "    import os\n",
    "    \n",
    "    # Option 1: Play a custom sound file if it exists (uncomment and set path)\n",
    "    # sound_file = 'notification.wav'  # Put your .wav file in the same directory\n",
    "    # if os.path.exists(sound_file):\n",
    "    #     winsound.PlaySound(sound_file, winsound.SND_FILENAME)\n",
    "    # else:\n",
    "    #     # Option 2: System beeps (default)\n",
    "    #     winsound.Beep(800, 200)  # 800 Hz for 200ms\n",
    "    #     winsound.Beep(1000, 200)  # 1000 Hz for 200ms\n",
    "    \n",
    "    # Default: System beeps\n",
    "    winsound.Beep(800, 200)  # 800 Hz for 200ms\n",
    "    winsound.Beep(1000, 200)  # 1000 Hz for 200ms\n",
    "    print(\"\\n🎉 Model training complete! Submission ready.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n🎉 Model training complete! Submission ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
