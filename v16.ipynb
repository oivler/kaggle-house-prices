{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - Advanced Regression Techniques\n",
    "## SCORE: .11915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 CPU cores (leaving 1 free)\n",
      "Train: (1460, 81), Test: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_jobs = max(1, os.cpu_count() - 1)\n",
    "print(f\"Using {n_jobs} CPU cores (leaving 1 free)\")\n",
    "\n",
    "data_dir = 'house-prices-advanced-regression-techniques'\n",
    "train = pd.read_csv(f'{data_dir}/train.csv')\n",
    "test = pd.read_csv(f'{data_dir}/test.csv')\n",
    "\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train['SalePrice'].copy()\n",
    "test_ids = test['Id'].copy()\n",
    "\n",
    "train_idx = len(train)\n",
    "all_data = pd.concat([train.drop('SalePrice', axis=1), test], ignore_index=True)\n",
    "\n",
    "y_train_log = np.log1p(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "\n",
    "none_cols = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "             'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', \n",
    "             'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "\n",
    "for col in none_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col].fillna('None', inplace=True)\n",
    "\n",
    "if 'LotFrontage' in all_data.columns:\n",
    "    all_data['LotFrontage'].fillna(all_data['LotFrontage'].median(), inplace=True)\n",
    "if 'MasVnrType' in all_data.columns:\n",
    "    all_data['MasVnrType'].fillna('None', inplace=True)\n",
    "if 'MasVnrArea' in all_data.columns:\n",
    "    all_data['MasVnrArea'].fillna(0, inplace=True)\n",
    "if 'Electrical' in all_data.columns:\n",
    "    all_data['Electrical'].fillna(all_data['Electrical'].mode()[0], inplace=True)\n",
    "if 'GarageYrBlt' in all_data.columns:\n",
    "    all_data['GarageYrBlt'].fillna(all_data['YearBuilt'], inplace=True)\n",
    "\n",
    "numerical_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if all_data[col].isnull().sum() > 0:\n",
    "        all_data[col].fillna(0, inplace=True)\n",
    "\n",
    "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if all_data[col].isnull().sum() > 0:\n",
    "        all_data[col].fillna(all_data[col].mode()[0], inplace=True)\n",
    "\n",
    "skewed_features = ['MiscVal', 'PoolArea', 'LotArea', '3SsnPorch', 'LowQualFinSF', \n",
    "                   'BsmtFinSF2', 'ScreenPorch', 'EnclosedPorch', 'MasVnrArea', \n",
    "                   'OpenPorchSF', 'LotFrontage', 'BsmtFinSF1', 'WoodDeckSF']\n",
    "for col in skewed_features:\n",
    "    if col in all_data.columns:\n",
    "        all_data[f'{col}_log'] = np.log1p(all_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(col in all_data.columns for col in ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']):\n",
    "    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "    all_data['TotalSF_log'] = np.log1p(all_data['TotalSF'])\n",
    "\n",
    "if all(col in all_data.columns for col in ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']):\n",
    "    all_data['TotalBathrooms'] = (all_data['FullBath'] + \n",
    "                                  all_data['HalfBath'] * 0.5 + \n",
    "                                  all_data['BsmtFullBath'] + \n",
    "                                  all_data['BsmtHalfBath'] * 0.5)\n",
    "\n",
    "if 'YrSold' in all_data.columns and 'YearBuilt' in all_data.columns:\n",
    "    all_data['HouseAge'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "    all_data['YearsSinceRemodel'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "    all_data['Remodeled'] = (all_data['YearBuilt'] != all_data['YearRemodAdd']).astype(int)\n",
    "    if 'GarageYrBlt' in all_data.columns:\n",
    "        all_data['GarageAge'] = all_data['YrSold'] - all_data['GarageYrBlt']\n",
    "        all_data['GarageAge'] = all_data['GarageAge'].fillna(0)\n",
    "\n",
    "if 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['HasBasement'] = (all_data['TotalBsmtSF'] > 0).astype(int)\n",
    "if 'GarageArea' in all_data.columns:\n",
    "    all_data['HasGarage'] = (all_data['GarageArea'] > 0).astype(int)\n",
    "if '2ndFlrSF' in all_data.columns:\n",
    "    all_data['Has2ndFloor'] = (all_data['2ndFlrSF'] > 0).astype(int)\n",
    "\n",
    "if 'OverallQual' in all_data.columns:\n",
    "    all_data['OverallQual2'] = all_data['OverallQual'] ** 2\n",
    "    if 'GrLivArea' in all_data.columns:\n",
    "        all_data['OverallQual_GrLivArea'] = all_data['OverallQual'] * all_data['GrLivArea']\n",
    "    if 'TotalBsmtSF' in all_data.columns:\n",
    "        all_data['OverallQual_TotalBsmtSF'] = all_data['OverallQual'] * all_data['TotalBsmtSF']\n",
    "    if 'GarageCars' in all_data.columns:\n",
    "        all_data['OverallQual_GarageCars'] = all_data['OverallQual'] * all_data['GarageCars']\n",
    "    if 'OverallCond' in all_data.columns:\n",
    "        all_data['OverallQual_OverallCond'] = all_data['OverallQual'] * all_data['OverallCond']\n",
    "\n",
    "if 'GrLivArea' in all_data.columns:\n",
    "    all_data['GrLivArea_log'] = np.log1p(all_data['GrLivArea'])\n",
    "    if 'TotalBathrooms' in all_data.columns:\n",
    "        all_data['AreaPerBath'] = all_data['GrLivArea'] / (all_data['TotalBathrooms'] + 0.1)\n",
    "    if 'TotRmsAbvGrd' in all_data.columns:\n",
    "        all_data['AreaPerRoom'] = all_data['GrLivArea'] / (all_data['TotRmsAbvGrd'] + 0.1)\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'GarageArea' in all_data.columns:\n",
    "    all_data['GarageAreaPerCar'] = all_data['GarageArea'] / (all_data['GarageCars'] + 0.1)\n",
    "\n",
    "if all(col in all_data.columns for col in ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'WoodDeckSF']):\n",
    "    all_data['TotalPorchSF'] = (all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + \n",
    "                                all_data['3SsnPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'OverallCond' in all_data.columns:\n",
    "    all_data['QualityScore'] = all_data['OverallQual'] * all_data['OverallCond']\n",
    "    all_data['QualityScore2'] = all_data['QualityScore'] ** 2\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'YearRemodAdd' in all_data.columns:\n",
    "    all_data['Remodeled'] = (all_data['YearBuilt'] != all_data['YearRemodAdd']).astype(int)\n",
    "    all_data['RemodelAge'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "\n",
    "if 'GrLivArea' in all_data.columns and 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['GrLivArea_TotalBsmtSF'] = all_data['GrLivArea'] * all_data['TotalBsmtSF']\n",
    "    all_data['GrLivArea_TotalBsmtSF_log'] = np.log1p(all_data['GrLivArea_TotalBsmtSF'])\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['OverallQual_GrLivArea_log'] = all_data['OverallQual'] * np.log1p(all_data['GrLivArea'])\n",
    "\n",
    "if 'TotalSF' in all_data.columns:\n",
    "    all_data['TotalSF2'] = all_data['TotalSF'] ** 2\n",
    "    all_data['TotalSF_sqrt'] = np.sqrt(all_data['TotalSF'])\n",
    "\n",
    "if 'GrLivArea' in all_data.columns:\n",
    "    all_data['GrLivArea_sqrt'] = np.sqrt(all_data['GrLivArea'])\n",
    "    all_data['GrLivArea_cbrt'] = np.power(all_data['GrLivArea'], 1/3)\n",
    "\n",
    "if 'LotArea' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['LotArea_GrLivArea_ratio'] = all_data['GrLivArea'] / (all_data['LotArea'] + 1)\n",
    "    all_data['LotArea_GrLivArea_diff'] = all_data['LotArea'] - all_data['GrLivArea']\n",
    "\n",
    "if 'BedroomAbvGrd' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['BedroomArea'] = all_data['GrLivArea'] / (all_data['BedroomAbvGrd'] + 1)\n",
    "\n",
    "if 'Fireplaces' in all_data.columns:\n",
    "    all_data['HasFireplace'] = (all_data['Fireplaces'] > 0).astype(int)\n",
    "\n",
    "if 'PoolArea' in all_data.columns:\n",
    "    all_data['HasPool'] = (all_data['PoolArea'] > 0).astype(int)\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'NeighborhoodEncoded' in all_data.columns:\n",
    "    all_data['OverallQual_Neighborhood'] = all_data['OverallQual'] * all_data['NeighborhoodEncoded']\n",
    "\n",
    "if 'TotalBathrooms' in all_data.columns and 'BedroomAbvGrd' in all_data.columns:\n",
    "    all_data['BathBedRatio'] = all_data['TotalBathrooms'] / (all_data['BedroomAbvGrd'] + 1)\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'TotalBathrooms' in all_data.columns:\n",
    "    all_data['OverallQual_TotalBathrooms'] = all_data['OverallQual'] * all_data['TotalBathrooms']\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'OverallQual' in all_data.columns:\n",
    "    all_data['YearBuilt_OverallQual'] = all_data['YearBuilt'] * all_data['OverallQual']\n",
    "\n",
    "if 'NeighborhoodEncoded' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['Neighborhood_GrLivArea'] = all_data['NeighborhoodEncoded'] * all_data['GrLivArea']\n",
    "\n",
    "if 'TotalSF' in all_data.columns and 'TotRmsAbvGrd' in all_data.columns:\n",
    "    all_data['TotalSF_per_Room'] = all_data['TotalSF'] / (all_data['TotRmsAbvGrd'] + 0.1)\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    all_data['GarageCars_TotalSF'] = all_data['GarageCars'] * all_data['TotalSF']\n",
    "\n",
    "if 'YearBuilt' in all_data.columns and 'NeighborhoodEncoded' in all_data.columns:\n",
    "    all_data['YearBuilt_Neighborhood'] = all_data['YearBuilt'] * all_data['NeighborhoodEncoded']\n",
    "\n",
    "if 'LotArea' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['LotUtilization'] = all_data['GrLivArea'] / (all_data['LotArea'] + 1)\n",
    "    all_data['LotUtilization'] = all_data['LotUtilization'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['LotUtilization'] = np.clip(all_data['LotUtilization'], 0, 10)\n",
    "\n",
    "if 'HouseAge' in all_data.columns and 'OverallQual' in all_data.columns:\n",
    "    all_data['AgeAdjustedQuality'] = all_data['OverallQual'] / (np.abs(all_data['HouseAge']) + 1)\n",
    "    all_data['AgeAdjustedQuality'] = all_data['AgeAdjustedQuality'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['AgeAdjustedQuality'] = np.clip(all_data['AgeAdjustedQuality'], 0, 10)\n",
    "\n",
    "if 'HouseAge' in all_data.columns and 'OverallCond' in all_data.columns:\n",
    "    all_data['AgeAdjustedCondition'] = all_data['OverallCond'] / (np.abs(all_data['HouseAge']) + 1)\n",
    "    all_data['AgeAdjustedCondition'] = all_data['AgeAdjustedCondition'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['AgeAdjustedCondition'] = np.clip(all_data['AgeAdjustedCondition'], 0, 10)\n",
    "\n",
    "if 'YearsSinceRemodel' in all_data.columns and 'Remodeled' in all_data.columns:\n",
    "    all_data['RemodelValue'] = all_data['Remodeled'] * (1.0 / (np.abs(all_data['YearsSinceRemodel']) + 1))\n",
    "    all_data['RemodelValue'] = all_data['RemodelValue'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['RemodelValue'] = np.clip(all_data['RemodelValue'], 0, 1)\n",
    "\n",
    "if 'TotalBsmtSF' in all_data.columns and 'BsmtFinSF1' in all_data.columns:\n",
    "    all_data['BasementFinishRatio'] = all_data['BsmtFinSF1'] / (all_data['TotalBsmtSF'] + 0.1)\n",
    "    all_data['BasementFinishRatio'] = all_data['BasementFinishRatio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['BasementFinishRatio'] = np.clip(all_data['BasementFinishRatio'], 0, 1)\n",
    "\n",
    "if 'TotalPorchSF' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['OutdoorLivingRatio'] = all_data['TotalPorchSF'] / (all_data['GrLivArea'] + 1)\n",
    "    all_data['OutdoorLivingRatio'] = all_data['OutdoorLivingRatio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['OutdoorLivingRatio'] = np.clip(all_data['OutdoorLivingRatio'], 0, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "quality_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "\n",
    "for col in quality_cols:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(quality_map).fillna(0).astype(int)\n",
    "\n",
    "exposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n",
    "if 'BsmtExposure' in all_data.columns:\n",
    "    all_data['BsmtExposure'] = all_data['BsmtExposure'].map(exposure_map).fillna(0).astype(int)\n",
    "\n",
    "finish_map = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0}\n",
    "for col in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(finish_map).fillna(0).astype(int)\n",
    "\n",
    "functional_map = {'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0}\n",
    "if 'Functional' in all_data.columns:\n",
    "    all_data['Functional'] = all_data['Functional'].map(functional_map).fillna(7).astype(int)\n",
    "\n",
    "if 'Neighborhood' in all_data.columns:\n",
    "    train_temp = all_data[:train_idx].copy()\n",
    "    train_temp['SalePrice'] = train_target\n",
    "    train_temp['SalePrice_log'] = np.log1p(train_target)\n",
    "    \n",
    "    neighborhood_stats = train_temp.groupby('Neighborhood')['SalePrice_log'].agg(['mean', 'std', 'count'])\n",
    "    global_mean = train_target.mean()\n",
    "    global_mean_log = np.log1p(global_mean)\n",
    "    \n",
    "    alpha = 5\n",
    "    neighborhood_encoded = (neighborhood_stats['mean'] * neighborhood_stats['count'] + global_mean_log * alpha) / (neighborhood_stats['count'] + alpha)\n",
    "    all_data['NeighborhoodEncoded'] = all_data['Neighborhood'].map(neighborhood_encoded.to_dict()).fillna(global_mean_log)\n",
    "    all_data['NeighborhoodEncoded_log'] = all_data['NeighborhoodEncoded']\n",
    "    \n",
    "    all_data['NeighborhoodStd'] = all_data['Neighborhood'].map(neighborhood_stats['std'].to_dict()).fillna(train_target.std())\n",
    "    all_data['NeighborhoodCount'] = all_data['Neighborhood'].map(neighborhood_stats['count'].to_dict()).fillna(0)\n",
    "\n",
    "if 'SaleType' in all_data.columns:\n",
    "    sale_type_map = {'New': 1, 'Con': 1, 'CWD': 0.8, 'ConLI': 0.7, 'WD': 0.5, \n",
    "                     'COD': 0.3, 'ConLw': 0.3, 'ConLD': 0.2, 'Oth': 0.1}\n",
    "    all_data['SaleTypeValue'] = all_data['SaleType'].map(sale_type_map).fillna(0.5)\n",
    "\n",
    "if 'SaleCondition' in all_data.columns:\n",
    "    sale_cond_map = {'Partial': 1.0, 'Normal': 0.8, 'Alloca': 0.7, 'Family': 0.6, \n",
    "                     'Abnorml': 0.4, 'AdjLand': 0.2}\n",
    "    all_data['SaleConditionValue'] = all_data['SaleCondition'].map(sale_cond_map).fillna(0.8)\n",
    "\n",
    "low_importance_features = ['Utilities', 'Street']\n",
    "for col in low_importance_features:\n",
    "    if col in all_data.columns:\n",
    "        all_data = all_data.drop(col, axis=1)\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_data[col] = le.fit_transform(all_data[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "if 'KitchenQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['KitchenQual_GrLivArea'] = all_data['KitchenQual'] * all_data['GrLivArea']\n",
    "\n",
    "if 'BsmtQual' in all_data.columns and 'TotalBsmtSF' in all_data.columns:\n",
    "    all_data['BsmtQual_TotalBsmtSF'] = all_data['BsmtQual'] * all_data['TotalBsmtSF']\n",
    "\n",
    "if 'ExterQual' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['ExterQual_GrLivArea'] = all_data['ExterQual'] * all_data['GrLivArea']\n",
    "\n",
    "if 'FireplaceQu' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['FireplaceQu_GrLivArea'] = all_data['FireplaceQu'] * all_data['GrLivArea']\n",
    "\n",
    "if 'GarageQual' in all_data.columns and 'GarageArea' in all_data.columns:\n",
    "    all_data['GarageQual_GarageArea'] = all_data['GarageQual'] * all_data['GarageArea']\n",
    "\n",
    "quality_features = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                    'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\n",
    "available_quality = [col for col in quality_features if col in all_data.columns]\n",
    "if len(available_quality) > 0:\n",
    "    all_data['TotalQualityScore'] = all_data[available_quality].sum(axis=1)\n",
    "\n",
    "if 'BsmtFinType1' in all_data.columns and 'BsmtFinSF1' in all_data.columns:\n",
    "    all_data['BasementFinishQuality'] = all_data['BsmtFinType1'] * all_data['BsmtFinSF1']\n",
    "\n",
    "if 'Functional' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['FunctionalValue'] = all_data['Functional'] * all_data['GrLivArea']\n",
    "\n",
    "if 'ExterQual' in all_data.columns and 'TotalPorchSF' in all_data.columns:\n",
    "    all_data['PorchQuality'] = all_data['ExterQual'] * all_data['TotalPorchSF']\n",
    "\n",
    "if 'SaleConditionValue' in all_data.columns and 'SaleTypeValue' in all_data.columns:\n",
    "    all_data['SaleValueFactor'] = all_data['SaleConditionValue'] * all_data['SaleTypeValue']\n",
    "\n",
    "if 'OverallQual' in all_data.columns and 'OverallCond' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['QualityConditionArea'] = (all_data['OverallQual'] + all_data['OverallCond']) * all_data['GrLivArea']\n",
    "\n",
    "if 'GarageCars' in all_data.columns and 'GarageArea' in all_data.columns:\n",
    "    all_data['GarageEfficiency'] = all_data['GarageArea'] / (all_data['GarageCars'] + 0.1)\n",
    "    all_data['GarageEfficiency'] = all_data['GarageEfficiency'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['GarageEfficiency'] = np.clip(all_data['GarageEfficiency'], 0, 1000)\n",
    "\n",
    "if 'TotRmsAbvGrd' in all_data.columns and 'TotalSF' in all_data.columns:\n",
    "    all_data['RoomDensity'] = all_data['TotRmsAbvGrd'] / (all_data['TotalSF'] + 1)\n",
    "    all_data['RoomDensity'] = all_data['RoomDensity'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['RoomDensity'] = np.clip(all_data['RoomDensity'], 0, 1)\n",
    "\n",
    "if 'BedroomAbvGrd' in all_data.columns and 'GrLivArea' in all_data.columns:\n",
    "    all_data['BedroomDensity'] = all_data['BedroomAbvGrd'] / (all_data['GrLivArea'] + 1)\n",
    "    all_data['BedroomDensity'] = all_data['BedroomDensity'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    all_data['BedroomDensity'] = np.clip(all_data['BedroomDensity'], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8 outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 150 features (from 153)\n"
     ]
    }
   ],
   "source": [
    "train_processed = all_data[:train_idx].copy()\n",
    "test_processed = all_data[train_idx:].copy()\n",
    "\n",
    "train_processed = train_processed.drop('Id', axis=1)\n",
    "test_processed = test_processed.drop('Id', axis=1)\n",
    "\n",
    "outliers_grliv = train_processed[(train_processed['GrLivArea'] > 4000) & (y_train_log < 12.5)].index\n",
    "\n",
    "Q1 = train_processed['GrLivArea'].quantile(0.25)\n",
    "Q3 = train_processed['GrLivArea'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = train_processed[(train_processed['GrLivArea'] < (Q1 - 3 * IQR)) | \n",
    "                                (train_processed['GrLivArea'] > (Q3 + 3 * IQR))].index\n",
    "\n",
    "z_scores = np.abs(stats.zscore(train_processed[['GrLivArea', 'TotalBsmtSF']].fillna(0)))\n",
    "outliers_z = train_processed[(z_scores > 4).any(axis=1)].index\n",
    "\n",
    "outliers = list(set(list(outliers_grliv) + list(outliers_iqr) + list(outliers_z)))\n",
    "train_processed = train_processed.drop(outliers)\n",
    "y_train_log = y_train_log.drop(outliers)\n",
    "print(f\"Removed {len(outliers)} outliers\")\n",
    "\n",
    "mi_scores = mutual_info_regression(train_processed.fillna(0), y_train_log, random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': train_processed.columns,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "rf_selector = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=n_jobs)\n",
    "rf_selector.fit(train_processed.fillna(0), y_train_log)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_processed.columns,\n",
    "    'rf_importance': rf_selector.feature_importances_\n",
    "})\n",
    "\n",
    "combined_importance = pd.merge(mi_df, feature_importance, on='feature')\n",
    "combined_importance['combined_score'] = (combined_importance['mi_score'] * 0.5 + \n",
    "                                      combined_importance['rf_importance'] * 0.5)\n",
    "combined_importance = combined_importance.sort_values('combined_score', ascending=False)\n",
    "\n",
    "important_features = combined_importance[combined_importance['combined_score'] > 0.00025]['feature'].tolist()\n",
    "train_processed = train_processed[important_features]\n",
    "test_processed = test_processed[important_features]\n",
    "\n",
    "train_processed = train_processed.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "test_processed = test_processed.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "for col in train_processed.columns:\n",
    "    if train_processed[col].dtype in [np.float64, np.float32]:\n",
    "        train_processed[col] = np.clip(train_processed[col], -1e10, 1e10)\n",
    "        test_processed[col] = np.clip(test_processed[col], -1e10, 1e10)\n",
    "\n",
    "print(f\"Selected {len(important_features)} features (from {len(combined_importance)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: 9 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "seeds = [42, 123, 456, 789, 2024, 999, 1337, 2023, 3141]\n",
    "all_rf_predictions = []\n",
    "\n",
    "for seed in seeds:\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=1200,\n",
    "        max_depth=25,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        random_state=seed,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    rf_model.fit(train_processed, y_train_log)\n",
    "    all_rf_predictions.append(np.expm1(rf_model.predict(test_processed)))\n",
    "\n",
    "rf_predictions = np.mean(all_rf_predictions, axis=0)\n",
    "print(f\"RF: {len(seeds)} seeds averaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: 9 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    all_xgb_predictions = []\n",
    "    for seed in seeds:\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=20000,\n",
    "            learning_rate=0.0018,\n",
    "            max_depth=6,\n",
    "            min_child_weight=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        xgb_model.fit(train_processed, y_train_log, verbose=False)\n",
    "        all_xgb_predictions.append(np.expm1(xgb_model.predict(test_processed)))\n",
    "    \n",
    "    xgb_predictions = np.mean(all_xgb_predictions, axis=0)\n",
    "    print(f\"XGB: {len(seeds)} seeds averaged\")\n",
    "except ImportError:\n",
    "    xgb_predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT: 9 seeds averaged\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    all_lgb_predictions = []\n",
    "    for seed in seeds:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=20000,\n",
    "            learning_rate=0.0018,\n",
    "            max_depth=6,\n",
    "            num_leaves=63,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=seed,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgb_model.fit(train_processed, y_train_log)\n",
    "        all_lgb_predictions.append(np.expm1(lgb_model.predict(test_processed)))\n",
    "    \n",
    "    lgb_predictions = np.mean(all_lgb_predictions, axis=0)\n",
    "    print(f\"LGB: {len(seeds)} seeds averaged\")\n",
    "except ImportError:\n",
    "    lgb_predictions = None\n",
    "\n",
    "cat_predictions = None\n",
    "try:\n",
    "    import catboost as cb\n",
    "    all_cat_predictions = []\n",
    "    for seed in seeds:\n",
    "        cat_model = cb.CatBoostRegressor(\n",
    "            iterations=20000,\n",
    "            learning_rate=0.0018,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=5,\n",
    "            loss_function='RMSE',\n",
    "            eval_metric='RMSE',\n",
    "            random_seed=seed,\n",
    "            verbose=False,\n",
    "            thread_count=n_jobs\n",
    "        )\n",
    "        cat_model.fit(train_processed, y_train_log, verbose=False)\n",
    "        all_cat_predictions.append(np.expm1(cat_model.predict(test_processed)))\n",
    "    \n",
    "    cat_predictions = np.mean(all_cat_predictions, axis=0)\n",
    "    print(f\"CAT: {len(seeds)} seeds averaged\")\n",
    "except Exception as e:\n",
    "    print(f\"CatBoost error: {type(e).__name__}: {str(e)}\")\n",
    "    print(\"Skipping CatBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Added 1039 pseudo-labeled samples\n",
      "Iteration 2: Added 961 pseudo-labeled samples\n"
     ]
    }
   ],
   "source": [
    "all_initial_preds = []\n",
    "if rf_predictions is not None:\n",
    "    all_initial_preds.append(rf_predictions)\n",
    "if xgb_predictions is not None:\n",
    "    all_initial_preds.append(xgb_predictions)\n",
    "if lgb_predictions is not None:\n",
    "    all_initial_preds.append(lgb_predictions)\n",
    "if cat_predictions is not None:\n",
    "    all_initial_preds.append(cat_predictions)\n",
    "\n",
    "if len(all_initial_preds) > 0:\n",
    "    initial_predictions = np.mean(all_initial_preds, axis=0)\n",
    "    pred_variance = np.var(all_initial_preds, axis=0)\n",
    "    pred_std = np.std(all_initial_preds, axis=0)\n",
    "    \n",
    "    ensemble_agreement = pred_std < (np.median(pred_std) * 1.6)\n",
    "    median_distance = np.abs(initial_predictions - np.median(initial_predictions)) < (np.std(initial_predictions) * 2.3)\n",
    "    test_confident = ensemble_agreement & median_distance\n",
    "    confident_indices = np.where(test_confident)[0]\n",
    "    \n",
    "    confidence_weights = 1.0 / (pred_std + 0.01)\n",
    "    confidence_weights = confidence_weights / confidence_weights.max()\n",
    "else:\n",
    "    initial_predictions = rf_predictions\n",
    "    test_confident = np.abs(initial_predictions - np.median(initial_predictions)) < (np.std(initial_predictions) * 2.2)\n",
    "    confident_indices = np.where(test_confident)[0]\n",
    "    confidence_weights = np.ones(len(initial_predictions))\n",
    "\n",
    "if len(confident_indices) > 200:\n",
    "    for iteration in range(2):\n",
    "        pseudo_train = test_processed.iloc[confident_indices].copy()\n",
    "        pseudo_target = initial_predictions[confident_indices]\n",
    "        pseudo_target_log = np.log1p(pseudo_target)\n",
    "        pseudo_weights = confidence_weights[confident_indices]\n",
    "        \n",
    "        train_enhanced = pd.concat([train_processed, pseudo_train], ignore_index=True)\n",
    "        y_enhanced = pd.concat([pd.Series(y_train_log), pd.Series(pseudo_target_log)], ignore_index=True)\n",
    "        sample_weights = pd.concat([pd.Series(np.ones(len(y_train_log))), pd.Series(pseudo_weights)], ignore_index=True)\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}: Added {len(confident_indices)} pseudo-labeled samples\")\n",
    "        \n",
    "        rf_enhanced = RandomForestRegressor(n_estimators=1200, max_depth=25, min_samples_split=5,\n",
    "                                            min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=n_jobs)\n",
    "        rf_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights)\n",
    "        rf_predictions = np.expm1(rf_enhanced.predict(test_processed))\n",
    "        \n",
    "        if xgb_predictions is not None:\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                xgb_enhanced = xgb.XGBRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                               min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                               gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs)\n",
    "                xgb_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights, verbose=False)\n",
    "                xgb_predictions = np.expm1(xgb_enhanced.predict(test_processed))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if lgb_predictions is not None:\n",
    "            try:\n",
    "                import lightgbm as lgb\n",
    "                lgb_enhanced = lgb.LGBMRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                                num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "                lgb_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights)\n",
    "                lgb_predictions = np.expm1(lgb_enhanced.predict(test_processed))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if cat_predictions is not None:\n",
    "            try:\n",
    "                import catboost as cb\n",
    "                cat_enhanced = cb.CatBoostRegressor(iterations=20000, learning_rate=0.0018, depth=6,\n",
    "                                                   l2_leaf_reg=5, loss_function='RMSE', eval_metric='RMSE',\n",
    "                                                   random_seed=42, verbose=False, thread_count=n_jobs)\n",
    "                cat_enhanced.fit(train_enhanced, y_enhanced, sample_weight=sample_weights, verbose=False)\n",
    "                cat_predictions = np.expm1(cat_enhanced.predict(test_processed))\n",
    "            except Exception as e:\n",
    "                print(f\"CatBoost pseudo-labeling error: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        if iteration < 2:\n",
    "            all_updated_preds = []\n",
    "            if rf_predictions is not None:\n",
    "                all_updated_preds.append(rf_predictions)\n",
    "            if xgb_predictions is not None:\n",
    "                all_updated_preds.append(xgb_predictions)\n",
    "            if lgb_predictions is not None:\n",
    "                all_updated_preds.append(lgb_predictions)\n",
    "            if cat_predictions is not None:\n",
    "                all_updated_preds.append(cat_predictions)\n",
    "            \n",
    "            if len(all_updated_preds) > 0:\n",
    "                updated_predictions = np.mean(all_updated_preds, axis=0)\n",
    "                pred_std = np.std(all_updated_preds, axis=0)\n",
    "                \n",
    "                ensemble_agreement = pred_std < (np.median(pred_std) * (1.5 - iteration * 0.1))\n",
    "                median_distance = np.abs(updated_predictions - np.median(updated_predictions)) < (np.std(updated_predictions) * (2.2 - iteration * 0.1))\n",
    "                test_confident = ensemble_agreement & median_distance\n",
    "                confident_indices = np.where(test_confident)[0]\n",
    "                initial_predictions = updated_predictions\n",
    "                \n",
    "                confidence_weights = 1.0 / (pred_std + 0.01)\n",
    "                confidence_weights = confidence_weights / confidence_weights.max()\n",
    "else:\n",
    "    print(\"Not enough confident predictions for pseudo-labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: 5 alphas averaged\n",
      "ElasticNet: 15 configs averaged\n",
      "GBR: 7 seeds averaged\n",
      "Non-linear stacking (XGB) OOF RMSE: 0.0957\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_processed)\n",
    "X_test_scaled = scaler.transform(test_processed)\n",
    "\n",
    "all_ridge_predictions = []\n",
    "for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    ridge_model = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge_model.fit(X_train_scaled, y_train_log)\n",
    "    all_ridge_predictions.append(np.expm1(ridge_model.predict(X_test_scaled)))\n",
    "\n",
    "all_elastic_predictions = []\n",
    "for alpha in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "    for l1_ratio in [0.3, 0.5, 0.7]:\n",
    "        elastic_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "        elastic_model.fit(X_train_scaled, y_train_log)\n",
    "        all_elastic_predictions.append(np.expm1(elastic_model.predict(X_test_scaled)))\n",
    "\n",
    "all_gbr_predictions = []\n",
    "for seed in seeds[:7]:\n",
    "    gbr_model = GradientBoostingRegressor(n_estimators=600, learning_rate=0.01, max_depth=5,\n",
    "                                          random_state=seed, subsample=0.8)\n",
    "    gbr_model.fit(X_train_scaled, y_train_log)\n",
    "    all_gbr_predictions.append(np.expm1(gbr_model.predict(X_test_scaled)))\n",
    "\n",
    "ridge_predictions = np.mean(all_ridge_predictions, axis=0)\n",
    "elastic_predictions = np.mean(all_elastic_predictions, axis=0)\n",
    "gbr_predictions = np.mean(all_gbr_predictions, axis=0)\n",
    "print(f\"Ridge: {len(all_ridge_predictions)} alphas averaged\")\n",
    "print(f\"ElasticNet: {len(all_elastic_predictions)} configs averaged\")\n",
    "print(f\"GBR: {len(all_gbr_predictions)} seeds averaged\")\n",
    "\n",
    "oof_predictions = np.zeros((len(train_processed), 7))\n",
    "test_predictions = np.zeros((len(test_processed), 7))\n",
    "\n",
    "for fold, (train_idx_fold, val_idx_fold) in enumerate(kf.split(train_processed)):\n",
    "    X_train_fold = train_processed.iloc[train_idx_fold]\n",
    "    X_val_fold = train_processed.iloc[val_idx_fold]\n",
    "    y_train_fold = y_train_log.iloc[train_idx_fold]\n",
    "    y_val_fold = y_train_log.iloc[val_idx_fold]\n",
    "    \n",
    "    scaler_fold = RobustScaler()\n",
    "    X_train_scaled_fold = scaler_fold.fit_transform(X_train_fold)\n",
    "    X_val_scaled_fold = scaler_fold.transform(X_val_fold)\n",
    "    X_test_scaled_fold = scaler_fold.transform(test_processed)\n",
    "    \n",
    "    rf_fold = RandomForestRegressor(n_estimators=1200, max_depth=25, min_samples_split=5,\n",
    "                                    min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=n_jobs)\n",
    "    rf_fold.fit(X_train_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 0] = rf_fold.predict(X_val_fold)\n",
    "    test_predictions[:, 0] += np.expm1(rf_fold.predict(test_processed)) / kf.n_splits\n",
    "    \n",
    "    if xgb_predictions is not None:\n",
    "        try:\n",
    "            import xgboost as xgb\n",
    "            xgb_fold = xgb.XGBRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                        min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs)\n",
    "            xgb_fold.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)\n",
    "            oof_predictions[val_idx_fold, 1] = xgb_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 1] += np.expm1(xgb_fold.predict(test_processed)) / kf.n_splits\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if lgb_predictions is not None:\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            lgb_fold = lgb.LGBMRegressor(n_estimators=20000, learning_rate=0.0018, max_depth=6,\n",
    "                                        num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=n_jobs, verbose=-1)\n",
    "            lgb_fold.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], \n",
    "                        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)])\n",
    "            oof_predictions[val_idx_fold, 2] = lgb_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 2] += np.expm1(lgb_fold.predict(test_processed)) / kf.n_splits\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    ridge_fold = Ridge(alpha=10.0, random_state=42)\n",
    "    ridge_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 3] = ridge_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 3] += np.expm1(ridge_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "    \n",
    "    elastic_fold = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42, max_iter=2000)\n",
    "    elastic_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 4] = elastic_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 4] += np.expm1(elastic_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "    \n",
    "    if cat_predictions is not None:\n",
    "        try:\n",
    "            import catboost as cb\n",
    "            cat_fold = cb.CatBoostRegressor(iterations=20000, learning_rate=0.0018, depth=6,\n",
    "                                           l2_leaf_reg=5, loss_function='RMSE', eval_metric='RMSE',\n",
    "                                           random_seed=42, verbose=False, thread_count=n_jobs)\n",
    "            cat_fold.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False)\n",
    "            oof_predictions[val_idx_fold, 5] = cat_fold.predict(X_val_fold)\n",
    "            test_predictions[:, 5] += np.expm1(cat_fold.predict(test_processed)) / kf.n_splits\n",
    "        except Exception as e:\n",
    "            if fold == 0:\n",
    "                print(f\"CatBoost stacking error: {type(e).__name__}: {str(e)}\")\n",
    "    \n",
    "    gbr_fold = GradientBoostingRegressor(n_estimators=600, learning_rate=0.01, max_depth=5,\n",
    "                                        random_state=42, subsample=0.8)\n",
    "    gbr_fold.fit(X_train_scaled_fold, y_train_fold)\n",
    "    oof_predictions[val_idx_fold, 6] = gbr_fold.predict(X_val_scaled_fold)\n",
    "    test_predictions[:, 6] += np.expm1(gbr_fold.predict(X_test_scaled_fold)) / kf.n_splits\n",
    "\n",
    "valid_cols = [i for i in range(7) if oof_predictions[:, i].sum() != 0]\n",
    "oof_stack = oof_predictions[:, valid_cols]\n",
    "test_stack = test_predictions[:, valid_cols]\n",
    "\n",
    "test_stack_log = np.log1p(test_stack)\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    meta_model = xgb.XGBRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    meta_model.fit(oof_stack, y_train_log)\n",
    "    meta_rmse = np.sqrt(np.mean((meta_model.predict(oof_stack) - y_train_log) ** 2))\n",
    "    print(f\"Non-linear stacking (XGB) OOF RMSE: {meta_rmse:.4f}\")\n",
    "    final_predictions = np.expm1(meta_model.predict(test_stack_log))\n",
    "except:\n",
    "    def objective(weights):\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        blend = np.dot(oof_stack, weights)\n",
    "        return np.sqrt(np.mean((blend - y_train_log) ** 2))\n",
    "    \n",
    "    initial_weights = np.ones(len(valid_cols)) / len(valid_cols)\n",
    "    bounds = [(0, 1) for _ in range(len(valid_cols))]\n",
    "    result = minimize(objective, initial_weights, method='SLSQP', bounds=bounds, \n",
    "                      constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    optimal_weights = result.x / result.x.sum()\n",
    "    print(f\"Optimal weights: {optimal_weights}\")\n",
    "    print(f\"OOF RMSE with optimal weights: {result.fun:.4f}\")\n",
    "    final_predictions = np.expm1(np.dot(test_stack_log, optimal_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal blend weights: [1.00000000e+00 2.22044605e-16 0.00000000e+00 0.00000000e+00]\n",
      "OOF RMSE with optimal blend: 0.1114\n",
      "Submission saved: submission.csv\n",
      "Final predictions range: 46629.12 - 493288.00\n"
     ]
    }
   ],
   "source": [
    "predictions_for_geom = [rf_predictions]\n",
    "if xgb_predictions is not None:\n",
    "    predictions_for_geom.append(xgb_predictions)\n",
    "if lgb_predictions is not None:\n",
    "    predictions_for_geom.append(lgb_predictions)\n",
    "if cat_predictions is not None:\n",
    "    predictions_for_geom.append(cat_predictions)\n",
    "if 'gbr_predictions' in locals():\n",
    "    predictions_for_geom.append(gbr_predictions)\n",
    "\n",
    "geometric_mean = np.exp(np.mean([np.log(pred + 1) for pred in predictions_for_geom], axis=0)) - 1\n",
    "\n",
    "simple_weighted = (rf_predictions * 0.08 + \n",
    "                  (xgb_predictions * 0.25 if xgb_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (lgb_predictions * 0.25 if lgb_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (cat_predictions * 0.25 if cat_predictions is not None else rf_predictions * 0.25) +\n",
    "                  (gbr_predictions * 0.17 if 'gbr_predictions' in locals() else rf_predictions * 0.17))\n",
    "\n",
    "median_pred = np.median([rf_predictions, \n",
    "                         xgb_predictions if xgb_predictions is not None else rf_predictions,\n",
    "                         lgb_predictions if lgb_predictions is not None else rf_predictions,\n",
    "                         cat_predictions if cat_predictions is not None else rf_predictions,\n",
    "                         gbr_predictions if 'gbr_predictions' in locals() else rf_predictions], axis=0)\n",
    "\n",
    "oof_base_predictions = {\n",
    "    'stacked': oof_stack.mean(axis=1) if len(valid_cols) > 0 else np.zeros(len(y_train_log)),\n",
    "    'geometric': np.zeros(len(y_train_log)),\n",
    "    'weighted': np.zeros(len(y_train_log)),\n",
    "    'median': np.zeros(len(y_train_log))\n",
    "}\n",
    "\n",
    "for fold, (train_idx_fold, val_idx_fold) in enumerate(kf.split(train_processed)):\n",
    "    fold_geom_preds = []\n",
    "    fold_weighted_preds = []\n",
    "    fold_median_preds = []\n",
    "    \n",
    "    rf_fold_pred = np.expm1(oof_predictions[val_idx_fold, 0])\n",
    "    fold_geom_preds.append(rf_fold_pred)\n",
    "    fold_weighted_preds.append(rf_fold_pred * 0.08)\n",
    "    fold_median_preds.append(rf_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 1].sum() > 0:\n",
    "        xgb_fold_pred = np.expm1(oof_predictions[val_idx_fold, 1])\n",
    "        fold_geom_preds.append(xgb_fold_pred)\n",
    "        fold_weighted_preds.append(xgb_fold_pred * 0.25)\n",
    "        fold_median_preds.append(xgb_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 2].sum() > 0:\n",
    "        lgb_fold_pred = np.expm1(oof_predictions[val_idx_fold, 2])\n",
    "        fold_geom_preds.append(lgb_fold_pred)\n",
    "        fold_weighted_preds.append(lgb_fold_pred * 0.25)\n",
    "        fold_median_preds.append(lgb_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 5].sum() > 0:\n",
    "        cat_fold_pred = np.expm1(oof_predictions[val_idx_fold, 5])\n",
    "        fold_geom_preds.append(cat_fold_pred)\n",
    "        fold_weighted_preds.append(cat_fold_pred * 0.25)\n",
    "        fold_median_preds.append(cat_fold_pred)\n",
    "    \n",
    "    if oof_predictions[val_idx_fold, 6].sum() > 0:\n",
    "        gbr_fold_pred = np.expm1(oof_predictions[val_idx_fold, 6])\n",
    "        fold_geom_preds.append(gbr_fold_pred)\n",
    "        fold_weighted_preds.append(gbr_fold_pred * 0.17)\n",
    "        fold_median_preds.append(gbr_fold_pred)\n",
    "    \n",
    "    if fold_geom_preds:\n",
    "        oof_base_predictions['geometric'][val_idx_fold] = np.exp(np.mean([np.log(pred + 1) for pred in fold_geom_preds], axis=0)) - 1\n",
    "        oof_base_predictions['weighted'][val_idx_fold] = np.sum(fold_weighted_preds, axis=0)\n",
    "        oof_base_predictions['median'][val_idx_fold] = np.median(fold_median_preds, axis=0)\n",
    "\n",
    "oof_base_predictions['stacked'] = np.expm1(oof_stack.mean(axis=1)) if len(valid_cols) > 0 else np.zeros(len(y_train_log))\n",
    "\n",
    "blend_oof_array = np.column_stack([\n",
    "    oof_base_predictions['stacked'],\n",
    "    oof_base_predictions['geometric'],\n",
    "    oof_base_predictions['weighted'],\n",
    "    oof_base_predictions['median']\n",
    "])\n",
    "\n",
    "def blend_objective(weights):\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    blend = np.dot(blend_oof_array, weights)\n",
    "    return np.sqrt(np.mean((np.log1p(blend) - y_train_log) ** 2))\n",
    "\n",
    "initial_blend_weights = np.array([0.50, 0.25, 0.15, 0.10])\n",
    "bounds = [(0, 1) for _ in range(4)]\n",
    "result_blend = minimize(blend_objective, initial_blend_weights, method='SLSQP', bounds=bounds,\n",
    "                        constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "optimal_blend_weights = result_blend.x / result_blend.x.sum()\n",
    "print(f\"Optimal blend weights: {optimal_blend_weights}\")\n",
    "print(f\"OOF RMSE with optimal blend: {result_blend.fun:.4f}\")\n",
    "\n",
    "final_blend = (optimal_blend_weights[0] * final_predictions +\n",
    "               optimal_blend_weights[1] * geometric_mean +\n",
    "               optimal_blend_weights[2] * simple_weighted +\n",
    "               optimal_blend_weights[3] * median_pred)\n",
    "\n",
    "max_price = np.expm1(y_train_log.max()) * 1.5\n",
    "min_price = np.expm1(y_train_log.min()) * 0.5\n",
    "\n",
    "final_blend = np.clip(final_blend, min_price, max_price)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'SalePrice': final_blend\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved: submission.csv\")\n",
    "print(f\"Final predictions range: {final_blend.min():.2f} - {final_blend.max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
